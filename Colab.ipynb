{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jot4roKujo/Predictive-Analytics/blob/main/Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD_BzkrHG4KR"
      },
      "source": [
        "# CORE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed2QtdILMsqp"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k-L9XuUJ_vd"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install pyecharts\n",
        "!pip install snapshot-selenium\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "GjCU35qq_jid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Clk4shV3uZ8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BK5BQx7NcdX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT9S7mp5Nuec"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql  import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf() \\\n",
        "          .setMaster('local[*]') \\\n",
        "          .setAppName('SB Analytics') \\\n",
        "          .set('spark.driver.memory', '20g') \\\n",
        "          .set('spark.driver.maxResultSizeSets', '8g')\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldB5XGS6VKgX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, col, count, sum as sum_, max as max_, min as min_, avg as avg_, round as round_,\\\n",
        "                                  lower, when, udf, mean as mean_, stddev as stddev_, array, struct, product, \\\n",
        "                                  first, last, lit, split, isnan, months_between, row_number, coalesce, size, exp, \\\n",
        "                                  explode, array_repeat, date_format, date_trunc, to_date, to_timestamp, broadcast\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "from pyspark.sql.functions import avg\n",
        "from functools import reduce\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
        "\n",
        "from pyecharts.globals import CurrentConfig, NotebookType\n",
        "from pyecharts.render import make_snapshot\n",
        "from pyecharts.options import *\n",
        "from pyecharts.charts import Bar, Grid\n",
        "from pyecharts import options as opts\n",
        "from pyecharts.commons.utils import JsCode\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "from IPython.display import display\n",
        "import streamlit as st\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.calibration import calibration_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCJFOgeNMYQ0"
      },
      "source": [
        "## Upload Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDr2Tr6fLni_"
      },
      "outputs": [],
      "source": [
        "dfIC = spark.read.load('/content/drive/MyDrive/db/ic')\n",
        "dfPatients = spark.read.load('/content/drive/MyDrive/db/patients')\n",
        "dfObservations = spark.read.load('/content/drive/MyDrive/db/observations')\n",
        "dfConditions = spark.read.load('/content/drive/MyDrive/db/conditions')\n",
        "#dfMedications = spark.read.load('/content/drive/MyDrive/db/medications')\n",
        "dfQuestionnaireResponses = spark.read.load('/content/drive/MyDrive/db/questionnaire_responses')\n",
        "dfResearchSubjects = spark.read.load('/content/drive/MyDrive/db/research_subjects')\n",
        "\n",
        "dfPatients = dfPatients.withColumnRenamed('id','Patient_id')\n",
        "dfObservations = dfObservations.withColumn('Patient_id',expr(\"substring(subject_reference, 9, length(subject_reference))\"))\n",
        "dfConditions = dfConditions.withColumn('Patient_id',expr(\"substring(subject_reference, 9, length(subject_reference))\"))\n",
        "dfQuestionnaireResponses = dfQuestionnaireResponses.withColumn('Patient_id',expr(\"substring(subject_reference, 9, length(subject_reference))\"))\n",
        "dfResearchSubjects = dfResearchSubjects.withColumnRenamed('id','Patient_id')\n",
        "\n",
        "#dfResearchSubjects.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgbf4VQcyg7H"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replace Null with Mean and 0\n",
        "\n"
      ],
      "metadata": {
        "id": "Wwqh_58yg8ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_nulls(df):\n",
        "    # Step 1: Identify numeric columns\n",
        "    numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (IntegerType, FloatType, DoubleType))]\n",
        "\n",
        "    # Step 2: Identify binary and continuous columns\n",
        "    binary_columns = []\n",
        "    continuous_columns = []\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        distinct_values = df.select(col).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        if set(distinct_values).issubset({0, 1, None}):\n",
        "            binary_columns.append(col)\n",
        "        else:\n",
        "            continuous_columns.append(col)\n",
        "\n",
        "    # Step 3: Replace nulls in binary columns with 0\n",
        "    for col in binary_columns:\n",
        "        df = df.withColumn(col, F.when(F.col(col).isNull(), F.lit(0)).otherwise(F.col(col)))\n",
        "\n",
        "    # Step 4: Replace nulls in continuous columns with the mean of the column\n",
        "    for col in continuous_columns:\n",
        "        mean_value = df.select(F.mean(F.col(col))).collect()[0][0]\n",
        "        df = df.withColumn(col, F.when(F.col(col).isNull(), F.lit(mean_value)).otherwise(F.col(col)))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "We794qrohCPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replace Null with 0"
      ],
      "metadata": {
        "id": "B8JLMlHkPGFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_null_with_zero(df):\n",
        "    # Step 1: Identify numerical columns\n",
        "    numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (IntegerType, FloatType, DoubleType))]\n",
        "\n",
        "    # Step 2: For each numerical column, replace Null with 0\n",
        "    for col in numeric_columns:\n",
        "        df = df.withColumn(col, F.when(F.col(col).isNull(), F.lit(0)).otherwise(F.col(col)))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "nMUo_aLoPItQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM5-nN34OfkI"
      },
      "source": [
        "### Standing Blood Pressure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FAoKh9WOjIH"
      },
      "outputs": [],
      "source": [
        "# STANDING Data\n",
        "\n",
        "# Preparing data\n",
        "def prepare_standing_bp_data(dfStandingBP):\n",
        "\n",
        "    dfStandingBP = dfStandingBP.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\n",
        "\n",
        "    # Daily grouping and count\n",
        "    dfStandingBPGrouped = dfStandingBP.groupBy('effectiveDateTime').agg(\n",
        "        count('Standing Systolic').alias('Systolic Readings'),\n",
        "        count('Standing Diastolic').alias('Diastolic Readings')\n",
        "    ).orderBy('effectiveDateTime')\n",
        "\n",
        "    return dfStandingBPGrouped\n",
        "\n",
        "# Creating Chart\n",
        "def create_standing_bp_chart(dfStandingBPGrouped):\n",
        "\n",
        "    pdfStandingBP = dfStandingBPGrouped.toPandas()\n",
        "    dates = pdfStandingBP['effectiveDateTime'].tolist()\n",
        "    systolic_readings = pdfStandingBP['Systolic Readings'].tolist()\n",
        "    diastolic_readings = pdfStandingBP['Diastolic Readings'].tolist()\n",
        "\n",
        "    bar = Bar()\n",
        "    bar.add_xaxis(dates)\n",
        "    bar.add_yaxis(\"Systolic Readings\", systolic_readings, color=\"#1f77b4\")\n",
        "    bar.add_yaxis(\"Diastolic Readings\", diastolic_readings, color=\"#ff7f0e\")\n",
        "    bar.set_global_opts(\n",
        "        title_opts=opts.TitleOpts(title=\"Daily Distribution of Standing Blood Pressure Readings\"),\n",
        "        xaxis_opts=opts.AxisOpts(name=\"Date\", axislabel_opts=opts.LabelOpts(is_show=False)),\n",
        "        yaxis_opts=opts.AxisOpts(name=\"Number of Readings\"),\n",
        "        legend_opts=opts.LegendOpts(\n",
        "            is_show=True,\n",
        "            pos_bottom=\"0%\",\n",
        "            pos_left=\"center\",\n",
        "            orient=\"horizontal\"\n",
        "        )\n",
        "    )\n",
        "    bar.set_series_opts(\n",
        "        label_opts=opts.LabelOpts(is_show=False)  # Disable labeling\n",
        "    )\n",
        "\n",
        "    return bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrRUzs1yPUQU"
      },
      "source": [
        "### Supine Blood Pressure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Esozh3LzPXN-"
      },
      "outputs": [],
      "source": [
        "# SUPINE Data\n",
        "\n",
        "# Preparing data\n",
        "def prepare_supine_bp_data(dfSupineBP):\n",
        "\n",
        "    dfSupineBP = dfSupineBP.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\n",
        "\n",
        "    # Raggruppare i dati per giorno e contare il numero di letture\n",
        "    dfSupineBPGrouped = dfSupineBP.groupBy('effectiveDateTime').agg(\n",
        "        count('Supine Systolic').alias('Systolic Readings'),\n",
        "        count('Supine Diastolic').alias('Diastolic Readings')\n",
        "    ).orderBy('effectiveDateTime')\n",
        "\n",
        "    return dfSupineBPGrouped\n",
        "\n",
        "# Creating Chart\n",
        "def create_supine_bp_chart(dfSupineBPGrouped):\n",
        "\n",
        "    pdfSupineBP = dfSupineBPGrouped.toPandas()\n",
        "    dates = pdfSupineBP['effectiveDateTime'].tolist()\n",
        "    systolic_readings = pdfSupineBP['Systolic Readings'].tolist()\n",
        "    diastolic_readings = pdfSupineBP['Diastolic Readings'].tolist()\n",
        "\n",
        "    bar = Bar()\n",
        "    bar.add_xaxis(dates)\n",
        "    bar.add_yaxis(\"Systolic Readings\", systolic_readings, color=\"#1f77b4\")\n",
        "    bar.add_yaxis(\"Diastolic Readings\", diastolic_readings, color=\"#ff7f0e\")\n",
        "    bar.set_global_opts(\n",
        "        title_opts=opts.TitleOpts(title=\"Daily Distribution of Supine Blood Pressure Readings\"),\n",
        "        xaxis_opts=opts.AxisOpts(name=\"Date\", axislabel_opts=opts.LabelOpts(is_show=False)),\n",
        "        yaxis_opts=opts.AxisOpts(name=\"Number of Readings\"),\n",
        "        legend_opts=opts.LegendOpts(\n",
        "            is_show=True,\n",
        "            pos_bottom=\"0%\",\n",
        "            pos_left=\"center\",\n",
        "            orient=\"horizontal\"\n",
        "        )\n",
        "    )\n",
        "    bar.set_series_opts(\n",
        "        label_opts=opts.LabelOpts(is_show=False)  # Disable labeling\n",
        "    )\n",
        "\n",
        "    return bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9GJo9QbXZsp"
      },
      "source": [
        "### Age Distribution + Conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_X6-Y5bXdJh"
      },
      "outputs": [],
      "source": [
        "def create_age_groups(df):\n",
        "    \"\"\"\n",
        "    Crea i gruppi di età nel DataFrame.\n",
        "    \"\"\"\n",
        "    return df.withColumn(\n",
        "        'AgeGroup',\n",
        "        when(col('Age') < 30, 'Less than 30')\n",
        "        .when((col('Age') >= 30) & (col('Age') < 40), '30-39')\n",
        "        .when((col('Age') >= 40) & (col('Age') < 50), '40-49')\n",
        "        .when((col('Age') >= 50) & (col('Age') < 60), '50-59')\n",
        "        .when((col('Age') >= 60) & (col('Age') < 70), '60-69')\n",
        "        .when((col('Age') >= 70) & (col('Age') < 80), '70-79')\n",
        "        .otherwise('More than 80')\n",
        "    ).withColumn(\n",
        "        'AgeGroupOrder',\n",
        "        when(col('Age') < 30, 1)\n",
        "        .when((col('Age') >= 30) & (col('Age') < 40), 2)\n",
        "        .when((col('Age') >= 40) & (col('Age') < 50), 3)\n",
        "        .when((col('Age') >= 50) & (col('Age') < 60), 4)\n",
        "        .when((col('Age') >= 60) & (col('Age') < 70), 5)\n",
        "        .when((col('Age') >= 70) & (col('Age') < 80), 6)\n",
        "        .otherwise(7)\n",
        "    )\n",
        "\n",
        "def sum_patients_per_condition(df):\n",
        "    \"\"\"\n",
        "    Somma i pazienti per condizione e gruppo di età.\n",
        "    \"\"\"\n",
        "    conditions = df.columns[2:-2]  # Escludere 'Patient_id', 'Age' e 'AgeGroupOrder'\n",
        "    df_summary = df.groupBy('AgeGroup', 'AgeGroupOrder').agg(\n",
        "        *[_sum(col(condition)).alias(condition) for condition in conditions]\n",
        "    ).orderBy('AgeGroupOrder')\n",
        "    return df_summary\n",
        "\n",
        "def prepare_data_for_echarts(df_summary):\n",
        "    \"\"\"\n",
        "    Prepara i dati per ECharts.\n",
        "    \"\"\"\n",
        "    pdf_summary = df_summary.toPandas()\n",
        "    age_groups = pdf_summary['AgeGroup'].tolist()\n",
        "    data = {condition: pdf_summary[condition].tolist() for condition in pdf_summary.columns if condition not in ['AgeGroup', 'AgeGroupOrder']}\n",
        "    return age_groups, data\n",
        "\n",
        "def create_stacked_bar_chart(age_groups, data):\n",
        "    \"\"\"\n",
        "    Crea un grafico a barre verticali impilate con ECharts.\n",
        "    \"\"\"\n",
        "    bar = Bar(init_opts=opts.InitOpts(height=\"450px\", width=\"700\"))\n",
        "    bar.add_xaxis(age_groups)\n",
        "    for condition, values in data.items():\n",
        "        bar.add_yaxis(condition, values, stack=\"stack1\")\n",
        "    bar.set_global_opts(\n",
        "        title_opts=opts.TitleOpts(\n",
        "            title=\"Sum of all active Patients per condition per Age group\",\n",
        "            pos_left=\"center\",\n",
        "            title_textstyle_opts=opts.TextStyleOpts(font_size=14)\n",
        "        ),\n",
        "        xaxis_opts=opts.AxisOpts(name=\"Age Groups\"),\n",
        "        yaxis_opts=opts.AxisOpts(name=\"Number of Patients\", axislabel_opts=opts.LabelOpts(formatter=\"{value}\")),\n",
        "        legend_opts=opts.LegendOpts(\n",
        "            is_show=True,\n",
        "            pos_right=\"0%\",\n",
        "            pos_top=\"center\",\n",
        "            orient=\"vertical\",\n",
        "            item_gap=10,\n",
        "            textstyle_opts=opts.TextStyleOpts(font_size=10, align=\"left\")\n",
        "        )\n",
        "    )\n",
        "    bar.set_series_opts(\n",
        "        bar_category_gap=\"30%\",\n",
        "        label_opts=opts.LabelOpts(is_show=False)  # Disabilita le etichette\n",
        "    )\n",
        "\n",
        "    grid = Grid()\n",
        "    grid.add(bar, grid_opts=opts.GridOpts(\n",
        "        pos_left=\"10%\",  # Aumenta il margine sinistro\n",
        "        pos_right=\"20%\",  # Aumenta il margine destro\n",
        "        pos_top=\"10%\",  # Aumenta il margine superiore\n",
        "        pos_bottom=\"10%\"  # Aumenta il margine inferiore\n",
        "    ))\n",
        "\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8lLVALTTQpz"
      },
      "source": [
        "### Age Distribution Normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrl8jVNBTSvH"
      },
      "outputs": [],
      "source": [
        "# Age distribution of recruited study participants\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def calculate_age_group_statistics(df, age_col=\"Age\"):\n",
        "    # Defining groups\n",
        "    bins = [29, 39, 49, 49, 59, 69, 79, 100]\n",
        "    labels = [\"0-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80+\"]\n",
        "\n",
        "    # New column for groups\n",
        "    df = df.withColumn(\"AgeGroup\",\n",
        "                       F.when((F.col(age_col) >= 0) & (F.col(age_col) <= 29), \"0-29\")\n",
        "                       .when((F.col(age_col) >= 30) & (F.col(age_col) <= 39), \"30-39\")\n",
        "                       .when((F.col(age_col) >= 40) & (F.col(age_col) <= 49), \"40-49\")\n",
        "                       .when((F.col(age_col) >= 50) & (F.col(age_col) <= 59), \"50-59\")\n",
        "                       .when((F.col(age_col) >= 60) & (F.col(age_col) <= 69), \"60-69\")\n",
        "                       .when((F.col(age_col) >= 70) & (F.col(age_col) <= 79), \"70-79\")\n",
        "                       .otherwise(\"80+\")\n",
        "                       )\n",
        "\n",
        "    # Calculating participants, mean and dev for every group\n",
        "    result = df.groupBy(\"AgeGroup\") \\\n",
        "               .agg(\n",
        "                   F.count(\"*\").alias(\"N\"),\n",
        "                   F.mean(age_col).alias(\"Mean\"),\n",
        "                   F.stddev(age_col).alias(\"StdDev\")\n",
        "               ) \\\n",
        "               .orderBy(\"AgeGroup\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0RC5RuDqUU"
      },
      "source": [
        "### Df related"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA68DasjXSaz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Get most recent data\n",
        "Observations Dataframe related\n",
        "'''\n",
        "def codeExtraction(df, code:str, new_name, whichCode = 'code_coding_code', whichValue='component_valueQuantity_value'):\n",
        "  tmp = df.filter(df[whichCode] == code)\n",
        "  tmp = tmp.select('effectiveDateTime', 'Patient_id', whichValue)\n",
        "  tmp_notNull = tmp.filter(tmp.effectiveDateTime.isNotNull()) # & (tmp[whichValue].isNotNull()\n",
        "  tmp_notNull = tmp_notNull.orderBy(tmp_notNull['effectiveDateTime'].desc())\n",
        "  recentData = tmp_notNull.groupBy('Patient_id').agg(first(whichValue).alias(new_name))\n",
        "  # print(\"Recent data for '\" + whichCode + \"'\")\n",
        "  # recentData.show()\n",
        "  return recentData\n",
        "#codeExtraction(dfObservations, '365458002', whichValue='valueCodeableConcept_coding_display', new_name='nuovo_nome').show(truncate=False)\n",
        "\n",
        "'''\n",
        "Get all data corresponding to a given code\n",
        "Observations Dataframe related\n",
        "'''\n",
        "def codeExtractionWithTime(df, code:str, new_name, whichCode = 'code_coding_code', whichValue='component_valueQuantity_value'):\n",
        "  tmp = df.filter(df[whichCode] == code)\n",
        "  tmp = tmp.select('effectiveDateTime', 'Patient_id', whichValue)\n",
        "  tmp_notNull = tmp.filter(tmp.effectiveDateTime.isNotNull()) # & (tmp[whichValue].isNotNull()\n",
        "  tmp_notNull = tmp_notNull.orderBy(tmp_notNull['effectiveDateTime'].desc())\n",
        "  return tmp_notNull.withColumnRenamed(whichValue, new_name).distinct()\n",
        "#codeExtraction(dfObservations, '365458002', whichValue='valueCodeableConcept_coding_display', new_name='nuovo_nome').show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWd1MVLHvA4Q"
      },
      "source": [
        "### Fill dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO44dfkvvCgQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  generates missing dates for each patient, based on patient's min_date and max_date\n",
        "'''\n",
        "def generatePersonalizedMissingDates(df):\n",
        "  #date_trunc --> truncate a date, e.g. with 'mm' parameters: 15/12/2021 --> 01/12/2021\n",
        "  all_dates_df = df.groupBy(\"Patient_id\").agg(\n",
        "      max_(to_date(\"effectiveDateTime\", \"dd/MM/yyyy\")).alias(\"max_date\"),\n",
        "      min_(to_date(\"effectiveDateTime\", \"dd/MM/yyyy\")).alias(\"min_date\")\n",
        "  ).select(\n",
        "      \"Patient_id\",\n",
        "      expr(\"sequence(min_date, max_date, interval 1 day)\").alias(\"effectiveDateTime\")\n",
        "  ).withColumn(\n",
        "      \"effectiveDateTime\", explode(\"effectiveDateTime\")\n",
        "  ).withColumn(\n",
        "      \"effectiveDateTime\",\n",
        "      date_format(\"effectiveDateTime\", \"yyyy-MM-dd\")\n",
        "  )\n",
        "  return all_dates_df.join(df, ['Patient_id', 'effectiveDateTime'], 'left')\n",
        "\n",
        "'''\n",
        "  fill null values in a column, using values of the same date, if there is not, it's replaced with the mean of the patient's values.\n",
        "'''\n",
        "def fillNullByDate(df, cols):\n",
        "    # uncomment if using more values for a timestamp\n",
        "    #means = df.groupBy(['Patient_id', 'effectiveDateTime']).agg(*(\n",
        "    #    mean_(x).alias(x+'_mean_tmp') for x in df.columns if x in cols\n",
        "    #))\n",
        "    meanByPatient = df.groupBy(['Patient_id']).agg(*(\n",
        "        mean_(x).alias(x+'_mean_tmp') for x in df.columns if x in cols\n",
        "    ))\n",
        "\n",
        "    #df = df.join(means, on=['effectiveDateTime', 'Patient_id'], how='left_outer')\n",
        "    #for c in cols:\n",
        "    #  df = df.withColumn(c, when(col(c).isNotNull(), col(c)).otherwise(col(c+'_mean_tmp'))).drop(c+'_mean_tmp')\n",
        "\n",
        "    df = df.join(meanByPatient, on=['Patient_id'], how='left_outer')\n",
        "    for c in cols:\n",
        "      df = df.withColumn(c, when(col(c).isNotNull(), col(c)).otherwise(col(c+'_mean_tmp'))).drop(c+'_mean_tmp')\n",
        "\n",
        "    return df.select('effectiveDateTime', 'Patient_id', *cols).orderBy('Patient_id', 'effectiveDateTime')\n",
        "\n",
        "'''\n",
        "  create, for every column in 'cols', a column with a percentage of trustness, based on the null values that are going to be imputed\n",
        "'''\n",
        "def imputedIndex(df, cols):\n",
        "  window_spec = Window.partitionBy('Patient_id')\n",
        "  total_rows = count('effectiveDateTime').over(window_spec)\n",
        "  for c in cols:\n",
        "    df = df.withColumn(c+'_imputed', (sum_(when(df[c].isNull(), 1).otherwise(0)).over(window_spec) / total_rows * 100))\n",
        "  return df\n",
        "def imputedIndexRowWise(df, cols, domain):\n",
        "  df = df.withColumn(domain+'_imputed', (sum(when(col(c).isNull(), 1).otherwise(0) for c in cols) / len(cols) * 100))\n",
        "  return df\n",
        "\n",
        "'''data = [\n",
        "    ('2022-01-01', 1111, 1, None, 3),\n",
        "    ('2022-01-01', 1111, 1, None, 3),\n",
        "    ('2022-01-01', 1111, None, None, 3),\n",
        "    ('2022-01-01', 2222, None, 2, None),\n",
        "    ('2022-01-01', 2222, None, 2, None),\n",
        "    ('2022-01-01', 2222, None, 2, None),\n",
        "    ('2022-01-01', 2222, None, 2, None)\n",
        "]\n",
        "colls = ['effectiveDateTime', 'Patient_id', 'col1', 'col2', 'col3']\n",
        "df = spark.createDataFrame(data, colls)\n",
        "\n",
        "result_df = imputedIndexRowWise(df, ['col1', 'col2', 'col3'], 'test')\n",
        "result_df.show()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DkmbQnoGNP"
      },
      "source": [
        "### Datasets length related"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NlTI1ycoIwe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  It average values if the dates are in a range of 'hours'\n",
        "  for example: 2021-01-01 16:00:00 and 2021-01-01 16:30:00 --> we average those values\n",
        "\n",
        "  Parameters:\n",
        "    df: dataframe\n",
        "    valuesColumnName: the column containing the values\n",
        "    hours: the range of hours\n",
        "'''\n",
        "def averageTooCloseMeasurements(df, valuesColumnName, hours = 3):\n",
        "  w = (Window()\n",
        "   .partitionBy(col(\"Patient_id\"))\n",
        "   .orderBy(col(\"effectiveDateTime\").cast(\"timestamp\").cast(\"long\"))\n",
        "   .rangeBetween(-3 * 3600, 0))\n",
        "  df = df.withColumn(valuesColumnName, mean_(valuesColumnName).over(w))\n",
        "\n",
        "  w_desc = (Window()\n",
        "    .partitionBy(col(\"Patient_id\"))\n",
        "    .orderBy(col(\"effectiveDateTime\").cast(\"timestamp\").cast(\"long\").desc())\n",
        "    .rangeBetween(-3 * 3600, 0))\n",
        "  df = df.withColumn(\"last\", first(\"effectiveDateTime\").over(w_desc))\n",
        "  df = df.filter(col('effectiveDateTime') == col('last')).drop('last')\n",
        "  df.orderBy('Patient_id', 'effectiveDateTime')\n",
        "  return df.orderBy('Patient_id', 'effectiveDateTime')\n",
        "\n",
        "'''\n",
        "  generate missing date for each patient, base on min_date and max_date calculated over the total 'effectiveDateTime' column\n",
        "'''\n",
        "def generateMissingDates(df):\n",
        "  all_dates_df = df.groupBy(\"Patient_id\").agg(\n",
        "      max_(to_date(\"effectiveDateTime\", \"dd/MM/yyyy\")).alias(\"max_date\"),\n",
        "      min_(to_date(\"effectiveDateTime\", \"dd/MM/yyyy\")).alias(\"min_date\")\n",
        "  ).select(\n",
        "      \"Patient_id\",\n",
        "      expr(\"sequence(min_date, max_date, interval 1 day)\").alias(\"effectiveDateTime\")\n",
        "  ).withColumn(\n",
        "      \"effectiveDateTime\", explode(\"effectiveDateTime\")\n",
        "  ).withColumn(\n",
        "      \"effectiveDateTime\",\n",
        "      date_format(\"effectiveDateTime\", \"yyyy-MM-dd\")\n",
        "  )\n",
        "  return all_dates_df.select('effectiveDateTime', 'Patient_id')\n",
        "\n",
        "\n",
        "'''\n",
        "  return a df with date occurrences\n",
        "'''\n",
        "def getDateOccurrences(df):\n",
        "  tmp = df.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\n",
        "  return tmp.groupBy('effectiveDateTime', 'Patient_id').agg(count('effectiveDateTime').alias('occurrences'))\n",
        "\n",
        "'''\n",
        "  It normalize a datasets in order to have a certain amount of daily measurements.\n",
        "  It is done by repeating the same values of a day by 'dailyMeasurements', if a day is not present we repeat the latest (or last) values\n",
        "\n",
        "  Parameters:\n",
        "    df: dataframe\n",
        "    dailyMeasurements: the desidered daily measurements\n",
        "'''\n",
        "def normalizeDatasetLength(df, dailyMeasurements):\n",
        "  df = df.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\n",
        "  cols = df.columns[2:]\n",
        "\n",
        "  all_dates = generateMissingDates(df)\n",
        "  all_dates = all_dates.withColumn('effectiveDateTime', explode(array_repeat('effectiveDateTime', dailyMeasurements))) \\\n",
        "                       .withColumn('effectiveDateTime', to_date('effectiveDateTime'))\n",
        "  all_dates = all_dates.exceptAll(df.select('effectiveDateTime', 'Patient_id'))#.withColumn('effectiveDateTime', to_date('effectiveDateTime')))\n",
        "  #all_dates = all_dates.withColumn('effectiveDateTime', to_timestamp('effectiveDateTime'))\n",
        "  #for c in cols:\n",
        "  #  all_dates = all_dates.withColumn(c, lit(None))\n",
        "  #all_dates.orderBy('Patient_id', 'effectiveDateTime').show()\n",
        "  df = df.unionByName(all_dates, allowMissingColumns=True).orderBy('Patient_id', 'effectiveDateTime')\n",
        "\n",
        "  #df = fillNullByDate(df, cols)\n",
        "\n",
        "  if dailyMeasurements > 1:\n",
        "    # adding an id column to avoid ambiguity with 'Patient_id'-'effectiveDateTime'\n",
        "    windowSpec  = Window.partitionBy(['effectiveDateTime', 'Patient_id']).orderBy('effectiveDateTime')\n",
        "    df = df.withColumn('id', row_number().over(windowSpec))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exF7UDJuDm1n"
      },
      "source": [
        "### Z-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv4PimreVgSb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Compute the personalized zscore for each value in 'valuesColumnName' based on the column 'id'\n",
        "\n",
        "  Parameters:\n",
        "    df: the dataframe on which to compute the zscore\n",
        "    valuesColumnName: the column containing the values\n",
        "    id: the column representing the singular patients\n",
        "\n",
        "  Return:\n",
        "    the original dataframe with a column for the computed z-score\n",
        "'''\n",
        "def computePersonalizedZScore(df, valueColumnName, idColumnName='Patient_id'):\n",
        "    windowSpec = Window().partitionBy(idColumnName)\n",
        "\n",
        "    mean_col = mean_(df[valueColumnName]).over(windowSpec).alias(valueColumnName + \"_mean\")\n",
        "    stddev_col = stddev_(df[valueColumnName]).over(windowSpec).alias(valueColumnName + \"_stddev\")\n",
        "\n",
        "    df = df.select('*', ((df[valueColumnName] - mean_col) / stddev_col).cast(\"double\").alias(valueColumnName+'_zscore'))\n",
        "\n",
        "    return df#.filter(df[valueColumnName+'_zscore'].isNotNull())\n",
        "\n",
        "'''\n",
        "  Compute z-score for each patient using mean and std calculated globally on the dataframe\n",
        "  Should use it if we have too few data for each patient.\n",
        "\n",
        "  Parameters:\n",
        "    df: the dataframe on which to compute the zscore\n",
        "    valuesColumnName: the column containing the values\n",
        "\n",
        "  Return:\n",
        "    the original dataframe with a column for the computed z-score\n",
        "'''\n",
        "def computeZScore(df, valuesColumnName, g_mean = None, g_stddev = None):\n",
        "    if g_stddev is None:\n",
        "      g_mean = df.agg(mean_(col(valuesColumnName))).collect()[0][0]\n",
        "      g_stddev = df.agg(stddev_(col(valuesColumnName))).collect()[0][0]\n",
        "      print('GMEAN and GSTDDEV')\n",
        "      print(g_mean)\n",
        "      print(g_stddev)\n",
        "\n",
        "    df = df.select('*', ((col(valuesColumnName) - lit(g_mean)) / lit(g_stddev)).cast(\"double\").alias(valuesColumnName+'_zscore'))\n",
        "\n",
        "    return df#.filter(df[valuesColumnName + '_zscore'].isNotNull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtugnzLqdAMf"
      },
      "source": [
        "### Performance Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO2ry6y5dCjo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  logistic function implementation (https://en.wikipedia.org/wiki/Logistic_function)\n",
        "  modified in order to get a performance score in [1,6]\n",
        "\n",
        "  Parameters:\n",
        "    x: value\n",
        "    x_0: the 'x-axis' value of the function's midpoint (should be the 'expected value', e.g. mean)\n",
        "    L: the upper bound (the maximum performance score)\n",
        "    k: the growth rate of the function or steepness of the curve\n",
        "'''\n",
        "def logistic_function(x, x_0, L = 5, k=1):\n",
        "  #return L / (1 + math.exp(-k*(x - x_0)))\n",
        "  if x is None:\n",
        "    return None\n",
        "  return (L / (1 + math.exp(-k*(x - x_0)))) + 1\n",
        "\n",
        "def reverse_logistic_function(x, x_0, L = 5, k=1):\n",
        "  if x is None:\n",
        "    return None\n",
        "  return L / (1 + math.exp(k*(x - x_0))) + 1\n",
        "\n",
        "'''\n",
        "  gaussian function implementation (https://en.wikipedia.org/wiki/Gaussian_function)\n",
        "  modified in order to get a performance score in [1,6]\n",
        "\n",
        "  Parameters:\n",
        "    x: value\n",
        "    a = the height of the bell curve\n",
        "    b = the position of the center of the peak (should be the 'expected value', e.g. mean)\n",
        "    c = the width of the bell (should be the 'standard deviation')\n",
        "'''\n",
        "def gaussian(x, b, c, a = 5):\n",
        "  if x is None:\n",
        "    return None\n",
        "  return (a * math.exp( - ((x-b)**2/(2 * c**2)) )) + 1\n",
        "\n",
        "logistic_function_udf = udf(logistic_function, DoubleType())\n",
        "reverse_logistic_function_udf = udf(reverse_logistic_function, DoubleType())\n",
        "gaussian_udf = udf(gaussian, DoubleType())\n",
        "\n",
        "'''\n",
        "  Compute the performance score for each value in 'zscoreColumn' based on which lower/upper risk is setted\n",
        "\n",
        "  Parameters:\n",
        "    df: the dataframe on which to compute the performance score\n",
        "    g_mean: the global mean\n",
        "    g_std: the global std\n",
        "    valuesColumnName: the column containing the values, if it's present the function calculate 'g_mean' and 'g_std' over this column\n",
        "    lowerRisk: the lower risk value\n",
        "    upperRisk: the upper risk value\n",
        "    zscoreColumn: the column containing the zscore values\n",
        "\n",
        "  Return:\n",
        "    the original dataframe with a column for the computed performance score\n",
        "'''\n",
        "def performanceScore(df, g_mean, g_std, valuesColumnName = None, lowerRisk = None, upperRisk = None, zscoreColumn = 'zscore', rename = 'performance score'):\n",
        "  if (lowerRisk is None) & (upperRisk is None):\n",
        "    print('give at least one risk value')\n",
        "    return None\n",
        "\n",
        "  # global mean and std\n",
        "  if valuesColumnName is not None:\n",
        "    g_mean = df.select(mean_(valuesColumnName).alias('mean_tmp')).head()['mean_tmp']\n",
        "    g_std = df.select(stddev_(valuesColumnName).alias('std_tmp')).head()['std_tmp']\n",
        "    print(valuesColumnName)\n",
        "    print('computed g_mean: ', g_mean)\n",
        "    print(' computed g_std: ', g_std)\n",
        "    print('\\n')\n",
        "\n",
        "  lZScore = None\n",
        "  uZScore = None\n",
        "\n",
        "  if upperRisk is None:\n",
        "    df = df.select('*', logistic_function_udf(col(zscoreColumn), lit(0)).alias(rename)) # g_mean in z-score == 0, or should i use the 'personal' mean or the 'recommended value'\n",
        "    lZScore = (lowerRisk-g_mean)/g_std\n",
        "  elif lowerRisk is None:\n",
        "    df = df.select('*',  reverse_logistic_function_udf(col(zscoreColumn), lit(0)).alias(rename))\n",
        "    uZScore = (upperRisk-g_mean)/g_std\n",
        "  else:\n",
        "    lZScore = (lowerRisk-g_mean)/g_std\n",
        "    uZScore = (upperRisk-g_mean)/g_std\n",
        "    g_std = (g_std-g_mean)/g_std\n",
        "    #print('g_std in zscore: ', g_std)\n",
        "    #print('avg of risks in zscore (b): ', (uZScore+lZScore)/2, '  g_std/3 (c): ', g_std/3)\n",
        "    df = df.select('*',  when(col(zscoreColumn).isNotNull(), gaussian_udf(df[zscoreColumn], lit((uZScore+lZScore)/2), lit(g_std/3))).alias(rename))\n",
        "\n",
        "  #print(\"risk value (lower): \", lZScore)\n",
        "  #print(\"risk value (upper): \", uZScore)\n",
        "\n",
        "  colName = zscoreColumn.split('_')[0]\n",
        "  return df.drop(colName+'_zscore')#colName, colName+'_mean', colName+'_stddev')\n",
        "\n",
        "'''\n",
        "  compute performance score over 'cols', using this formula: [1 - average(cols)]\n",
        "  we use it for 'conditions' in each domain\n",
        "'''\n",
        "def conditionsPerformanceScore(df, cols, name='domain_conditions_ps'):\n",
        "  n = len(cols)\n",
        "  df = df.withColumn(name, 1 - (sum(col(cols[idx]) for idx in range(n))/n))\n",
        "  # convert range [0,1] to [1,6]\n",
        "  return df.withColumn(name, (((col(name) - 0) * (6 - 1)) / (1 - 0)) + 1).drop(*cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuuCFILtFMOm"
      },
      "source": [
        "# Patients' Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JGMAXmzs1JK"
      },
      "source": [
        "## Educational Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZj2IP0Mhx14"
      },
      "outputs": [],
      "source": [
        "dfEducational = codeExtraction(dfObservations, '365458002', 'Educational Status', whichValue = 'valueCodeableConcept_coding_display')\n",
        "#dfEducational.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ9Gv5XNBR3F"
      },
      "outputs": [],
      "source": [
        "#dfObservations.select('effectiveDateTime', 'valueQuantity_code', 'subject_reference', 'code_coding_code', 'code_coding_display', 'valueQuantity_value')\\\n",
        "#.filter(col('valueQuantity_code').isNotNull()).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIcMfaeFBpJq"
      },
      "source": [
        "## Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzheeNhu7_xu"
      },
      "outputs": [],
      "source": [
        "today = datetime.date.today()\n",
        "\n",
        "active_patients = dfPatients.dropDuplicates(['Patient_id'])\n",
        "num_active_patients = active_patients.count()\n",
        "dfAge = active_patients.withColumn(\"Age\", (months_between(col(\"current_date\"), col(\"birthDate\")) / 12).cast('int'))\n",
        "dfAge = dfAge.select('Patient_id', 'Age').distinct()\n",
        "#print(f\"Number of active patients: {num_active_patients}\") #1639\n",
        "#dfAge.orderBy(col(\"Age\").asc()).show(n=200,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S34VzzA30HWu"
      },
      "source": [
        "## Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqicYlhp0JHS"
      },
      "outputs": [],
      "source": [
        "# code_coding_code 263495000\n",
        "\n",
        "dfGender = dfObservations.filter(dfObservations.code_coding_code == '263495000') \\\n",
        "          .withColumnRenamed('valueCodeableConcept_coding_display', 'Gender') \\\n",
        "          .select('Patient_id', 'Gender').distinct()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPUgguxgYbwp"
      },
      "source": [
        "## Questionnaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlHhrhEjJq60"
      },
      "outputs": [],
      "source": [
        "# questionnaire list\n",
        "#dfQuestionnaires = dfQuestionnaireResponses.select('questionnaire').distinct()\n",
        "#dfQuestionnaires.show(100, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM_10idRSP9v"
      },
      "source": [
        "### RGA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHbviLWISRlo"
      },
      "outputs": [],
      "source": [
        "scoresName = ['frailScore', 'sarcopeniaScore', 'riskWeightLossScore', 'RCS', 'snaqScore']\n",
        "dfRGA = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%RGA%')) & (dfQuestionnaireResponses.item_linkId.isin(scoresName)))\n",
        "dfRGA = dfRGA.dropDuplicates(['id', 'item_linkId']).select('id', 'meta_lastUpdated', 'Patient_id', 'item_linkId', 'item_answer_valueInteger')\n",
        "dfRGA = dfRGA.withColumnRenamed('meta_lastUpdated', 'effectiveDateTime')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbb1BOKJnTO8"
      },
      "source": [
        "### FES-I, ABC, FGA, Mini-BESTest, RAPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmd7fgsinXIt"
      },
      "outputs": [],
      "source": [
        "'''dfFES = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%fes%')) & (dfQuestionnaireResponses.item_linkId == 'score'))\n",
        "dfABC = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%ABC%')) & (dfQuestionnaireResponses.item_linkId == 'score'))\n",
        "dfFGA = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%FGA%')) & (dfQuestionnaireResponses.item_linkId == 'score'))\n",
        "dfMiniBEST = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%Mini-BESTest%')) & (dfQuestionnaireResponses.item_linkId == 'score'))\n",
        "# 3 patients\n",
        "\n",
        "rapa_scores = ['scoreRapa1', 'scoreRapa2']\n",
        "dfRAPA = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%RAPA%')) & (dfQuestionnaireResponses.item_linkId.isin(rapa_scores)))\n",
        "#dfRAPA.show()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW1KcFQ2msx1"
      },
      "source": [
        "### GHABP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD6Bsvybmr7t"
      },
      "outputs": [],
      "source": [
        "dfGHABP = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%GHABP%')) \\\n",
        "                                          & (dfQuestionnaireResponses.item_linkId == 'Total_score'))\n",
        "dfGHABP = dfGHABP.withColumn('GHABP Score', coalesce(dfGHABP.item_answer_valueInteger, dfGHABP.item_answer_valueDecimal))\n",
        "dfGHABP = dfGHABP.dropDuplicates().withColumnRenamed('meta_lastUpdated', 'effectiveDateTime') \\\n",
        "                 .select('effectiveDateTime', 'Patient_id', 'GHABP Score')\n",
        "#dfGHABP.show()\n",
        "# questionnaire of only the patient that has 'Hearing Loss'\n",
        "# 5 rows - 5 patient\n",
        "# but not all the patient with HL has done this questionnaire\n",
        "# if it's not done should i put 0 as a score??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTSorwXcYMvQ"
      },
      "source": [
        "## Sleep Quality Scale - SQS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG6MkxpE8bEF"
      },
      "outputs": [],
      "source": [
        "dfSQS = dfQuestionnaireResponses.filter(col('questionnaire').like(\"%sleep-item%\"))\n",
        "#dfSQS.orderBy('Patient_id').show()\n",
        "dfSQS = dfSQS.groupby('Patient_id').agg(sum_('item_answer_valueInteger').alias('SQS'))\n",
        "#dfSQS.orderBy('Patient_id').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXseWaAaYPVh"
      },
      "source": [
        "## Geriatric Depression Scale - GDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RObA48wtYOKd"
      },
      "outputs": [],
      "source": [
        "# >= 5 equals to depression : https://geriatrictoolkit.missouri.edu/cog/GDS_SHORT_FORM.PDF\n",
        "# 15 questions, 0-14 are the question, the 15th is the total score\n",
        "dfGDS = dfQuestionnaireResponses.filter((col('questionnaire').like(\"%GDS%\")) & (col('item_linkId') == 15)).dropDuplicates()\n",
        "dfGDS = dfGDS.withColumnRenamed('item_answer_valueInteger', 'GDS Score').withColumnRenamed('meta_lastUpdated', 'effectiveDateTime') \\\n",
        "             .select('effectiveDateTime', 'Patient_id','GDS Score') \\\n",
        "             .orderBy('Patient_id', 'effectiveDateTime')\n",
        "#dfGDS.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msNkIpsyaYzn"
      },
      "source": [
        "# Patient info extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPOKBCK9fKTd"
      },
      "outputs": [],
      "source": [
        "# codeExtraction(df, code:str, new_name, whichCode = 'code_coding_code', whichValue='component_valueQuantity_value')\n",
        "\n",
        "dfGender = codeExtraction(dfObservations, '263495000', 'gender', whichValue='valueCodeableConcept_coding_display')\n",
        "dfEducationalStatus = codeExtraction(dfObservations, '365458002', 'Educational Status', whichValue='valueCodeableConcept_coding_display')\n",
        "dfHousehold = codeExtraction(dfObservations, '365481000', 'Household Composition', whichValue='valueCodeableConcept_coding_display')\n",
        "dfSmoker = codeExtraction(dfObservations, '72166-2', 'Smoking Status', whichValue='valueCodeableConcept_coding_display')\n",
        "#dfPhysicalActivity = codeExtraction(dfObservations, '68130003', 'Physical Activity', whichValue='valueQuantity_value') # mins/wk\n",
        "dfBMI = codeExtraction(dfObservations, '39156-5', 'BMI', whichValue='valueQuantity_value') # Kg/m2\n",
        "dfAlcoholIntake = codeExtraction(dfObservations, '443315005', 'Alcohol Intake', whichValue='valueInteger') # Number of alcohol units consumed on typical drinking day\n",
        "#dfImpairedCognition = codeExtraction(dfObservations, '386806002', 'impaired_cognition', whichValue='valueCodeableConcept_coding_display')\n",
        "\n",
        "dfPatientInfo = dfAge \\\n",
        "          .join(dfGender, 'Patient_id', 'full') \\\n",
        "          .join(dfEducationalStatus, 'Patient_id', 'full') \\\n",
        "          .join(dfHousehold, 'Patient_id', 'full') \\\n",
        "          .join(dfSmoker, 'Patient_id', 'full') \\\n",
        "          .join(dfBMI, 'Patient_id', 'full') \\\n",
        "          .join(dfAlcoholIntake, 'Patient_id', 'full') \\\n",
        "          .join(dfGDS, 'Patient_id', 'full') \\\n",
        "          #.join(dfPhysicalActivity, 'Patient_id', 'full') \\\n",
        "          #.join(dfSQS, 'Patient_id', 'full') \\\n",
        "          #.join(dfImpairedCognition, 'Patient_id', 'full') \\\n",
        "          #.join(dfDepression, 'Patient_id', 'full') \\\n",
        "#dfPatientInfo.filter(col('BMI').isNotNull()).drop('effectiveDateTime').dropDuplicates(['Patient_id']).show(truncate=False)\n",
        "\n",
        "dfPatientInfo.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NasG-x_-5xwz"
      },
      "source": [
        "# Patient Info Analisys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URvad7etl7vR"
      },
      "outputs": [],
      "source": [
        "def analyze_patient_data(df: DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Esegue delle analitiche descrittive sui dati del DataFrame dei pazienti.\n",
        "\n",
        "    Args:\n",
        "    - df: DataFrame di PySpark contenente le informazioni sui pazienti.\n",
        "\n",
        "    Returns:\n",
        "    - None. La funzione stampa i risultati delle analitiche e le tabelle filtrate.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filtra i pazienti tra i 50 e i 60 anni che fumano attivamente\n",
        "    active_smokers = df.filter(\n",
        "        (col('age').between(60, 70)) &\n",
        "        (col('Smoking Status') == 'Current every day smoker')\n",
        "    ).dropDuplicates(['Patient_id'])  # Rimuove duplicati basati su 'Patient_id'\n",
        "\n",
        "    num_active_smokers = active_smokers.count()\n",
        "    print(f\"Numero di pazienti tra i 60 e i 70 anni che fumano attivamente: {num_active_smokers}\")\n",
        "    active_smokers.show(n=num_active_smokers, truncate=False)  # Stampa la tabella filtrata per i fumatori attivi\n",
        "\n",
        "    # Filtra i pazienti che hanno fumato in passato e ora consumano almeno una unità di alcol al giorno\n",
        "    past_smokers_alcohol = df.filter(\n",
        "        (col('Smoking Status').isin('Former smoker', 'Current some day smoker')) &\n",
        "        (col('Alcohol Intake') >= 1)\n",
        "    ).dropDuplicates(['Patient_id'])  # Rimuove duplicati basati su 'Patient_id'\n",
        "\n",
        "    num_past_smokers_alcohol = past_smokers_alcohol.count()\n",
        "    print(f\"Numero di pazienti che hanno fumato in passato e ora consumano almeno una unità di alcol al giorno: {num_past_smokers_alcohol}\")\n",
        "    past_smokers_alcohol.show(n=num_past_smokers_alcohol, truncate=False)  # Stampa la tabella filtrata per fumatori passati che consumano alcol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMZ6LfQnz78"
      },
      "outputs": [],
      "source": [
        "#analyze_patient_data(dfPatientInfo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56i9QIDtYXJA"
      },
      "source": [
        "# Conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ20_NXKej19"
      },
      "source": [
        "## Conditions encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYKZ0meGeiu2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  return 'yes' or 'no' depending if the 'x' is contained in 'positivePatients'\n",
        "'''\n",
        "@udf(\"int\")\n",
        "def EncodeInList (x, positivePatients):\n",
        "  if x in positivePatients:\n",
        "    return 1\n",
        "  else:\n",
        "     return 0\n",
        "\n",
        "'''\n",
        "  label encoding for conditions\n",
        "'''\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "def labelEncoding(df, colName):\n",
        "  indexer = StringIndexer(inputCol=colName, outputCol='encoded')\n",
        "  df = indexer.fit(df).transform(df)\n",
        "  return df.drop(colName).withColumnRenamed('encoded', colName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYw4W3KkNVAS"
      },
      "source": [
        "## Locomotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jS3KWj1NY2Z"
      },
      "source": [
        "### Number of Falls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0BHRSup1Ha"
      },
      "outputs": [],
      "source": [
        "# Number of falls last year (so if data retrieved in 2022, falls are referred to 2021) - 391002003\n",
        "dfLastYearFalls = dfObservations.filter((dfObservations.code_coding_code == '391002003'))\n",
        "dfLastYearFalls = codeExtraction(dfObservations, code='391002003', new_name=\"Number of Falls in Last Year\", whichCode='code_coding_code', whichValue='valueInteger')\n",
        "#dfLastYearFalls.orderBy('Patient_id').show(100, truncate=False)\n",
        "#dfLastYearFalls.select('Patient_id', 'effectiveDateTime').distinct().groupBy('Patient_id').count().orderBy('Patient_id').show(100, truncate=False)\n",
        "\n",
        "# Tendency to fall - 279992002    all NULL\n",
        "#dfFallTendency = dfConditions.filter((dfConditions.code_coding_code == '279992002'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8alKhro3UJYq"
      },
      "source": [
        "### Physical Activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcS0VZSjUbq3"
      },
      "outputs": [],
      "source": [
        "# Number of steps - 55423-8\n",
        "dfNumberSteps = codeExtractionWithTime(dfObservations, code='55423-8', new_name=\"Number of steps\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "#dfNumberSteps.orderBy('Patient_id', 'effectiveDateTime').show()\n",
        "# sum of all daily steps\n",
        "dfNumberSteps = dfNumberSteps.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                  .groupBy(['effectiveDateTime', 'Patient_id']).agg(sum_('Number of steps').alias('Number of steps'))\n",
        "#dfNumberSteps.orderBy('Patient_id', 'effectiveDateTime').show()\n",
        "\n",
        "\n",
        "# Walked Distance - 41953-1\n",
        "# [meter] or when NULL is [kilometer] --> transform all in 'meter'\n",
        "dfDistanceWalked = dfObservations.filter(dfObservations.code_coding_code == '41953-1').select('effectiveDateTime', 'Patient_id', 'valueQuantity_unit', col('valueQuantity_value').alias('Walked Distance'))\n",
        "dfDistanceWalked = dfDistanceWalked.withColumn('Walked Distance', \\\n",
        "                                               when(col('valueQuantity_unit').isNull(), \\\n",
        "                                                    col('Walked Distance')*1000)).select('effectiveDateTime', 'Patient_id', 'Walked Distance')\n",
        "#dfDistanceWalked = codeExtractionWithTime(dfObservations, code='41953-1', new_name=\"Walked Distance (meter)\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "dfDistanceWalked = dfDistanceWalked.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                    .groupBy(['effectiveDateTime', 'Patient_id']).agg(mean_('Walked Distance').alias('Walked Distance'))\n",
        "#dfDistanceWalked.orderBy('Patient_id', 'effectiveDateTime').show()\n",
        "\n",
        "\n",
        "'''dfNumberSteps = computePersonalizedZScore(dfNumberSteps, 'Number of steps')\n",
        "dfNumberSteps = performanceScore(dfNumberSteps, 6565, 1530, lowerRisk = 4000, zscoreColumn='Number of steps_zscore', rename = 'Number of steps_ps')\n",
        "#dfNumberSteps.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "\n",
        "dfDistanceWalked = computePersonalizedZScore(dfDistanceWalked, 'Walked Distance')\n",
        "dfDistanceWalked = performanceScore(dfDistanceWalked, 3939, 918, lowerRisk = 2400, zscoreColumn='Walked Distance_zscore', rename = 'Walked Distance_ps') # conversion from steps to km: steps*0,6\n",
        "'''\n",
        "'''dfPhysicalActivity = dfNumberSteps.select('effectiveDateTime', 'Patient_id', 'Number of steps_ps', 'id') \\\n",
        "                          .join(dfDistanceWalked.select('effectiveDateTime', 'Patient_id', 'Walked Distance_ps', 'id'), \\\n",
        "                           ['Patient_id', 'effectiveDateTime', 'id'], 'full')\n",
        "dfPhysicalActivity.show(10, truncate=False)'''\n",
        "#dfPhysicalActivity.na.drop().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRoWjbq7LF3Z"
      },
      "outputs": [],
      "source": [
        "#logisticFunctionPlot(3505, 6565, 1530, 3000)\n",
        "#plotPatientPerformanceScore(dfNumberSteps, '1065442966', 'Number of steps_ps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ESIByW6sxj"
      },
      "source": [
        "### Speed Gait/Individual Autonomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0CQaB606xYW"
      },
      "outputs": [],
      "source": [
        "# 5 questionnaires: RGA, FES-I, ABC, FGA, Mini-BEST and RAPA score\n",
        "# only RGA has substantial data\n",
        "# we can use: frailScore, sarcopeniaScore\n",
        "# https://www.slu.edu/medicine/internal-medicine/geriatric-medicine/aging-successfully/pdfs/rga_form_4-6-17.pdf\n",
        "dfFrail = dfRGA.filter(dfRGA.item_linkId == 'frailScore').withColumnRenamed('item_answer_valueInteger', 'Frail Score') \\\n",
        "                .select('effectiveDateTime', 'Patient_id', 'Frail Score')\n",
        "dfSarcopenia = dfRGA.filter(dfRGA.item_linkId == 'sarcopeniaScore').withColumnRenamed('item_answer_valueInteger', 'Sarcopenia Score') \\\n",
        "                .select('effectiveDateTime', 'Patient_id', 'Sarcopenia Score')\n",
        "\n",
        "'''\n",
        "  >= 3    --> Frail\n",
        "  1 or 2  --> pre frail\n",
        "'''\n",
        "#dfFrail = computePersonalizedZScore(dfFrail, 'Frail Score')\n",
        "#dfFrail = performanceScore(dfFrail, 0, 0, 'Frail Score', lowerRisk=3, zscoreColumn='Frail Score_zscore', rename = 'Frail Score_ps')\n",
        "#dfFrail.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "\n",
        "'''\n",
        "  >= 4  --> sarcopenia\n",
        "'''\n",
        "#dfSarcopenia = computePersonalizedZScore(dfSarcopenia, 'Sarcopenia Score')\n",
        "#dfSarcopenia = performanceScore(dfSarcopenia, 0, 0, 'Sarcopenia Score', lowerRisk = 4, zscoreColumn='Sarcopenia Score_zscore', rename = 'Sarcopenia Score_ps') # conversion from steps to km: steps*0,6\n",
        "#dfSarcopenia.show()\n",
        "\n",
        "# swaying gait - 69021004\n",
        "#dfObservations.filter(dfObservations.code_coding_code == '1160').select('component_code_coding_display').distinct().show(truncate=False)\n",
        "\n",
        "# 228449008 - Time spent exercising\n",
        "#dfObservations.filter(dfObservations.component_code_coding_code == '228450008').show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7Cn_yzDbQQk"
      },
      "source": [
        "## Sensory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CisH_QljYXY"
      },
      "source": [
        "### Hearing Capacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj14PyEAc7Cs"
      },
      "outputs": [],
      "source": [
        "# Family history of hearing loss - 439750006\n",
        "dfHearLossFamilyHistory = codeExtraction(dfObservations, code='439750006', new_name=\"Hearing Loss Family History\", whichCode='code_coding_code', whichValue='valueCodeableConcept_coding_display')\n",
        "\n",
        "# Hearing loss - 15188001\n",
        "\n",
        "sensoryConditions = [\n",
        "    ('Hearing Loss', '15188001'), # 'confirmed' or 'No history of hearing loss'\n",
        "    ('BPPV', '111541001'),\n",
        "    ('Vertigo', '399153001'),\n",
        "    ('Dizziness', '404640003'),\n",
        "    ('Meniere', '13445001')\n",
        "]\n",
        "#'verificationStatus_coding_code' all to NULL --> if in Conditions than they have the condition\n",
        "\n",
        "list_patient = dfConditions.filter(dfConditions.code_coding_code == sensoryConditions[0][1]) \\\n",
        "                          .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "dfSensoryConditions = dfObservations.select('Patient_id').distinct().withColumn(sensoryConditions[0][0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "for x in sensoryConditions[1:]:\n",
        "  list_patient = dfConditions\\\n",
        "                      .filter(dfConditions.code_coding_code == x[1]) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "  dfSensoryConditions = dfSensoryConditions.withColumn(x[0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "  #dfSensoryConditions = labelEncoding(dfSensoryConditions, x[0])\n",
        "dfSensoryConditions = conditionsPerformanceScore(dfSensoryConditions, dfSensoryConditions.columns[1:], 'Sensory_conditions_ps')\n",
        "\n",
        "# Does user hearing aid - 285055002\n",
        "# missing data of the patient 'using hearing aid'\n",
        "dfUseHearingAid = codeExtraction(dfObservations, code='285055002', new_name=\"Use Hearing Aid\", whichCode='code_coding_code', whichValue='valueCodeableConcept_coding_display')\n",
        "#dfUseHearingAid.select('Use Hearing Aid').distinct().show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMn657okycNd"
      },
      "source": [
        "## Psychological"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39NCZKRyhMf"
      },
      "source": [
        "### Sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgHHrsi-yeOf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Sleep Disorder - 39898005\n",
        "#dfSleepDisorderNegative = codeExtraction(dfObservations, code='39898005', new_name='Sleep Disorder', whichValue='valueCodeableConcept_coding_display')\n",
        "#dfSleepDisorderConfirmed = dfConditions.filter(dfConditions.code_coding_code == '39898005').dropDuplicates(['Patient_id']).select('Patient_id', 'verificationStatus_coding_code').withColumnRenamed('verificationStatus_coding_code', 'Sleep Disorder')\n",
        "#dfSleepDisorder = dfSleepDisorderNegative.union(dfSleepDisorderConfirmed)\n",
        "list_patient = dfConditions.filter(dfConditions.code_coding_code == '39898005') \\\n",
        "                          .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "dfSleepDisorder = dfObservations.select('Patient_id').distinct().withColumn(\"Sleep Disorder\", EncodeInList(\"Patient_id\", lit(list_patient)))\n",
        "#dfSleepDisorder = labelEncoding(dfSleepDisorder, 'Sleep Disorder')\n",
        "\n",
        "\n",
        "Recommended level (lower bound)\n",
        "total sleep time: 7-8 hours ---- mean: 7.38h --- std: 0.88  https://www.researchgate.net/figure/Sleep-duration-hours-mean-standard-deviation-in-each-of-the-four-questionnaires_tbl1_344226131\n",
        "light sleep: no minimum required, so? but can't find an upper bound\n",
        "deep sleep: 105 minutes minimum or 1.75 hours ---- mean: ---- std:\n",
        "\n",
        "\n",
        "# Light sleep duration [hours] - 93830-8\n",
        "dfLightSleepDuration = codeExtractionWithTime(dfObservations, code='93830-8', new_name='Light Sleep Duration', whichValue='valueQuantity_value')\n",
        "dfLightSleepDuration = dfLightSleepDuration.withColumn('effectiveDateTime', to_date('effectiveDateTime')) \\\n",
        "                        .groupBy(['Patient_id', 'effectiveDateTime']).agg(max_('Light Sleep Duration').alias('Light Sleep Duration'))\n",
        "\n",
        "# Deep sleep duration [hours] - 93831-6\n",
        "dfDeepSleepDuration = codeExtractionWithTime(dfObservations, code='93831-6', new_name='Deep Sleep Duration', whichValue='valueQuantity_value')\n",
        "dfDeepSleepDuration = dfDeepSleepDuration.withColumn('effectiveDateTime', to_date('effectiveDateTime')) \\\n",
        "                        .groupBy(['Patient_id', 'effectiveDateTime']).agg(max_('Deep Sleep Duration').alias('Deep Sleep Duration'))\n",
        "\n",
        "# Sleep duration [hours] - 93832-4\n",
        "#dfSleepDuration = dfObservations.filter((dfObservations.code_coding_code == '93832-4'))\n",
        "dfSleepDuration = codeExtractionWithTime(dfObservations, code='93832-4', new_name='Sleep Duration', whichValue='valueQuantity_value')\n",
        "dfSleepDuration = dfSleepDuration.withColumn('effectiveDateTime', to_date('effectiveDateTime')) \\\n",
        "                        .groupBy(['Patient_id', 'effectiveDateTime']).agg(max_('Sleep Duration').alias('Sleep Duration'))\n",
        "\n",
        "#dfLightSleepDuration.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "#dfDeepSleepDuration.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "#dfSleepDuration.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "\n",
        "# zscores\n",
        "#dfLightSleepDuration = computePersonalizedZScore(dfLightSleepDuration, 'Light Sleep Duration')\n",
        "#dfLightSleepDuration = performanceScore(dfLightSleepDuration, 7.38, 0.88, 'Light Sleep Duration', lowerRisk = 0, zscoreColumn='Light Sleep Duration_zscore', rename = 'Light Sleep Duration_ps')\n",
        "#dfLightSleepDuration.show()\n",
        "\n",
        "#dfDeepSleepDuration = computePersonalizedZScore(dfDeepSleepDuration, 'Deep Sleep Duration')\n",
        "#dfDeepSleepDuration = performanceScore(dfDeepSleepDuration, 0, 0, 'Deep Sleep Duration', lowerRisk = 1.75, zscoreColumn='Deep Sleep Duration_zscore', rename = 'Deep Sleep Duration_ps')\n",
        "#dfDeepSleepDuration.show()\n",
        "\n",
        "#dfSleepDuration = computePersonalizedZScore(dfSleepDuration, 'Sleep Duration')\n",
        "#dfSleepDuration = performanceScore(dfSleepDuration, 0, 0, 'Sleep Duration', lowerRisk = 7, zscoreColumn='Sleep Duration_zscore', rename = 'Sleep Duration_ps')\n",
        "#dfSleepDuration.show()\n",
        "\n",
        "dfSleep = dfSleepDuration.select('effectiveDateTime', 'Patient_id', 'Sleep Duration_ps') \\\n",
        "          .join(dfLightSleepDuration.select('effectiveDateTime', 'Patient_id', 'Light Sleep Duration_ps'), \\\n",
        "           ['Patient_id', 'effectiveDateTime'], 'full') \\\n",
        "          .join(dfDeepSleepDuration.select('effectiveDateTime', 'Patient_id', 'Deep Sleep Duration_ps'), \\\n",
        "           ['Patient_id', 'effectiveDateTime'], 'full')\n",
        "#dfSleep.show(10)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgQlWs0YMAeP"
      },
      "outputs": [],
      "source": [
        "#logisticFunctionPlot(6.30, 7.38, 0.88, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNZlMMK4zm-"
      },
      "source": [
        "### Lifestyle Habits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxtQ85BC44vb"
      },
      "outputs": [],
      "source": [
        "# no data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxRTzmC45Jme"
      },
      "source": [
        "### Mood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voBqB_tn5Irb"
      },
      "outputs": [],
      "source": [
        "# Activity - no data? menu of the mobile app\n",
        "# Humidity and Light levels - no data? coming from smart home devices\n",
        "\n",
        "# Sleep - same as 'Sleep' variable\n",
        "\n",
        "# Depression --> 35489007\n",
        "list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == '35489007') & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "dfDepression = dfObservations.select('Patient_id').distinct().withColumn(\"Depression\", EncodeInList(\"Patient_id\", lit(list_patient)))\n",
        "#dfDepression = labelEncoding(dfDepression, 'Depression')\n",
        "\n",
        "# Anxiety 197480006\n",
        "list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == '197480006') & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "dfAnxiety = dfObservations.select('Patient_id').distinct().withColumn(\"Anxiety\", EncodeInList(\"Patient_id\", lit(list_patient)))\n",
        "#dfAnxiety = labelEncoding(dfAnxiety, 'Anxiety')\n",
        "\n",
        "# PHQ 9 questionnaire\n",
        "# >= 10 --> moderate to severe depression : https://www.demenzemedicinagenerale.net/images/test/PHQ-9_Ok_20-2-2016.pdf\n",
        "dfPHQ9 = dfQuestionnaireResponses.filter(dfQuestionnaireResponses.questionnaire.like('%PHQ-9Questionnaire%')) \\\n",
        "                                         .withColumnRenamed('meta_lastUpdated', 'effectiveDateTime') \\\n",
        "                                         .dropDuplicates(['id', 'effectiveDateTime', 'Patient_id', 'item_linkId']) \\\n",
        "                                         .groupBy('effectiveDateTime', 'Patient_id').agg(sum_('item_answer_valueInteger').alias('PHQ9 Score')) \\\n",
        "                                         .select('effectiveDateTime', 'Patient_id', 'PHQ9 Score') \\\n",
        "                                         .orderBy('Patient_id', 'effectiveDateTime')\n",
        "#dfPHQ9 = computeZScore(dfPHQ9, 'PHQ9 Score')\n",
        "#dfPHQ9 = performanceScore(dfPHQ9, 0, 0, 'PHQ9 Score', lowerRisk=10, zscoreColumn = 'PHQ9 Score_zscore', rename = 'PHQ9 Score_ps')\n",
        "\n",
        "# GDS questionnaire\n",
        "# >= 5 equals to depression : https://geriatrictoolkit.missouri.edu/cog/GDS_SHORT_FORM.PDF\n",
        "# 15 questions, 0-14 are the question, the 15th is the total score\n",
        "dfGDS = dfQuestionnaireResponses.filter((col('questionnaire').like(\"%GDS%\")) & (col('item_linkId') == 15)).dropDuplicates()\n",
        "dfGDS = dfGDS.withColumnRenamed('item_answer_valueInteger', 'GDS Score').withColumnRenamed('meta_lastUpdated', 'effectiveDateTime') \\\n",
        "             .select('effectiveDateTime', 'Patient_id','GDS Score') \\\n",
        "             .orderBy('Patient_id', 'effectiveDateTime')\n",
        "#dfGDS = computeZScore(dfGDS, 'GDS Score')\n",
        "#dfGDS = performanceScore(dfGDS, 0, 0, 'GDS Score', lowerRisk=5, zscoreColumn = 'GDS Score_zscore', rename = 'GDS Score_ps')\n",
        "\n",
        "#dfDepression.show(5)\n",
        "#dfAnxiety.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzTl0SlXKI_T"
      },
      "source": [
        "## Psychological conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QUo7vvqKIVg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "dfPsychologicalConditions = dfSleepDisorder.join(dfDepression, ['Patient_id']).join(dfAnxiety, ['Patient_id'])\n",
        "dfPsychologicalConditions = conditionsPerformanceScore(dfPsychologicalConditions, ['Sleep Disorder', 'Depression', 'Anxiety'], 'Psychological_conditions_ps')\n",
        "#dfPsychologicalConditions.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YASDF-6tAi1h"
      },
      "source": [
        "# Cognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiRzFuV4A2XE"
      },
      "source": [
        "## Medication/Cognitive Games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBP4qZwIA7F-"
      },
      "outputs": [],
      "source": [
        "# no data??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXm9UwxoApRc"
      },
      "source": [
        "## Cognitive Decline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BX1UvyTAoHk"
      },
      "outputs": [],
      "source": [
        "# MoCA questionnaire\n",
        "dfMOCA = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%MOCA%')) & (dfQuestionnaireResponses.item_linkId == 'score')) \\\n",
        "          .dropDuplicates(['id'])\n",
        "dfMOCA = dfMOCA.select('meta_lastUpdated', 'Patient_id', 'item_answer_valueInteger') \\\n",
        "          .withColumnRenamed('item_answer_valueInteger', 'MOCA Score').withColumnRenamed('meta_lastUpdated', 'effectiveDateTime')\n",
        "#dfMOCA = computeZScore(dfMOCA, 'MOCA Score')\n",
        "# global mean and std: https://www.elsevier.es/en-revista-neurologia-english-edition--495-articulo-standardised-results-montreal-cognitive-assessment-S2173580822000517#:~:text=The%20mean%20(SD)%20overall%20MoCA,t%20%3D%200.372%20%5BP%20%3D%20.\n",
        "# >= 26 normale (< 26 at risk)\n",
        "#dfMOCA = performanceScore(dfMOCA, 24.1, 3.13, lowerRisk = 26, zscoreColumn='MOCA Score_zscore', rename = 'MOCA Score_ps')\n",
        "#dfMOCA.orderBy('Patient_id', 'effectiveDateTime').show(10)\n",
        "\n",
        "#  RGA questionnaire -- RCS (Rapid Cognitive Screen) score\n",
        "# <= 7 --- Mild cognitive impairment to dementia : https://www.slu.edu/medicine/internal-medicine/geriatric-medicine/aging-successfully/pdfs/rga_form_4-6-17.pdf\n",
        "dfRCS = dfRGA.filter(dfRGA.item_linkId == 'RCS').withColumnRenamed('item_answer_valueInteger', 'RCS Score') \\\n",
        "                .select('effectiveDateTime', 'Patient_id', 'RCS Score')\n",
        "#dfRCS = computeZScore(dfRCS, 'RCS Score')\n",
        "#dfRCS = performanceScore(dfRCS, 0, 0, 'RCS Score', lowerRisk=7, zscoreColumn='RCS Score_zscore', rename = 'RCS Score_ps')\n",
        "#dfRCS.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "\n",
        "# Impaired Cognition - 386806002\n",
        "list_patient = list(dfObservations.filter((dfObservations.code_coding_display == 'Impaired cognition') & \\\n",
        "                                           (dfObservations.valueCodeableConcept_coding_display != 'Normal cognition')).distinct().toPandas()['Patient_id'])\n",
        "list_patient.extend(list(dfConditions.filter((dfConditions.code_coding_display == 'Impaired cognition') & \\\n",
        "                                              (dfConditions.verificationStatus_coding_code == 'confirmed')).distinct().toPandas()['Patient_id']))\n",
        "dfImpairedCognition = dfObservations.withColumn(\"History of Mild Cognitive Impairment\", EncodeInList(\"Patient_id\", lit(list_patient)))\\\n",
        "                            .select('Patient_id','History of Mild Cognitive Impairment')\\\n",
        "                            .dropDuplicates(['Patient_id','History of Mild Cognitive Impairment'])\n",
        "#dfImpairedCognition = labelEncoding(dfImpairedCognition, 'History of Mild Cognitive Impairment')\n",
        "#dfImpairedCognition.show(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdpPnQF9LyyK"
      },
      "source": [
        "## Cognition conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xzgXiEKL1iw"
      },
      "outputs": [],
      "source": [
        "dfCognitionConditions = conditionsPerformanceScore(dfImpairedCognition, ['History of Mild Cognitive Impairment'], 'Cognition_conditions_ps')\n",
        "#dfCognitionConditions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ujmYbEJuZa"
      },
      "source": [
        "# Vitality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAppa2GzJxtX"
      },
      "source": [
        "## Weight Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zksJN8s7JweI"
      },
      "outputs": [],
      "source": [
        "# MNA questionnaire\n",
        "'''\n",
        "valutazione MNA long form\n",
        "24-30   : normal nutritional status (>= 24)\n",
        "17-23.5 : at risk of malnutrition\n",
        "< 17    : malnourished\n",
        "\n",
        "MNA-SF: short form (our should be this):\n",
        "  >= 12 : normal nutritional status\n",
        "  8-11  : at risk of malnutrition\n",
        "  0-7   : malnourished\n",
        "'''\n",
        "dfMNA = dfQuestionnaireResponses.filter((dfQuestionnaireResponses.questionnaire.like('%mna%')) & (dfQuestionnaireResponses.item_linkId == 'score'))\n",
        "dfMNA = dfMNA.dropDuplicates(['id']).select('meta_lastUpdated', 'Patient_id', 'item_answer_valueInteger') \\\n",
        "              .withColumnRenamed('item_answer_valueInteger', 'MNA Score').withColumnRenamed('meta_lastUpdated', 'effectiveDateTime')\n",
        "\n",
        "# RGA questionnaire --> SNAQ score (Simplified Nutritional Assessment Questionnaire)\n",
        "'''\n",
        "snaqScore <= 14 : significant risk of at least 5% weight loss within 6 months\n",
        "'''\n",
        "dfSNAQ = dfRGA.filter(dfRGA.item_linkId == 'snaqScore').dropDuplicates(['id']) \\\n",
        "              .select('effectiveDateTime', 'Patient_id', 'item_answer_valueInteger') \\\n",
        "              .withColumnRenamed('item_answer_valueInteger', 'SNAQ score')\n",
        "\n",
        "# Body weight - 29463-7\n",
        "dfBodyWeight = codeExtractionWithTime(dfObservations, code='29463-7', new_name='Body Weight', whichValue='valueQuantity_value')\n",
        "\n",
        "# compute zscore and perfomance score\n",
        "# global mean and std: https://pubmed.ncbi.nlm.nih.gov/25809801/\n",
        "#dfMNA = computeZScore(dfMNA, 'MNA Score')\n",
        "#dfMNA = performanceScore(dfMNA, 9.8, 2.4, lowerRisk = 12, zscoreColumn='MNA Score_zscore', rename = 'MNA Score_ps')\n",
        "#dfMNA.orderBy('Patient_id', 'effectiveDateTime').show(10)\n",
        "\n",
        "# global mean and std: https://www.researchgate.net/publication/370439474_Validity_and_Reliability_of_the_Persian_Version_of_the_Council_on_Nutrition_Appetite_Questionnaire_and_Its_Simplified_Version_in_Iranian_Community-Dwelling_Older_Adults\n",
        "#dfSNAQ = computeZScore(dfSNAQ, 'SNAQ Score')\n",
        "#dfSNAQ = performanceScore(dfSNAQ, 14.28, 2.34, lowerRisk = 14, zscoreColumn='SNAQ Score_zscore', rename = 'SNAQ Score_ps')\n",
        "#dfSNAQ.orderBy('Patient_id', 'effectiveDateTime').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9yQGJEI7Gbq"
      },
      "source": [
        "## Hearth Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkvJ8w58pwQY"
      },
      "outputs": [],
      "source": [
        "# is this an assessment?\n",
        "#dfObservations.filter(dfObservations.code_coding_code == '68999-2').orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "\n",
        "# Average Heart Rate [b/min] - 55425-3\n",
        "dfAvgHeartRate = codeExtractionWithTime(dfObservations, code='55425-3', new_name=\"Average Heart Rate\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "dfAvgHeartRate = dfAvgHeartRate.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                    .groupBy(['effectiveDateTime', 'Patient_id']).agg(mean_('Average Heart Rate').alias('Average Heart Rate'))\n",
        "\n",
        "# Max heart rate [b/min] - 55426-1\n",
        "dfMaxHeartRate = codeExtractionWithTime(dfObservations, code='55426-1', new_name=\"Max Heart Rate\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "dfMaxHeartRate = dfMaxHeartRate.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                    .groupBy(['effectiveDateTime', 'Patient_id']).agg(mean_('Max Heart Rate').alias('Max Heart Rate'))\n",
        "\n",
        "# Min heart rate [b/min] - 96935-2\n",
        "dfMinHeartRate = codeExtractionWithTime(dfObservations, code='96935-2', new_name=\"Min Heart Rate\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "dfMinHeartRate = dfMinHeartRate.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                    .groupBy(['effectiveDateTime', 'Patient_id']).agg(mean_('Min Heart Rate').alias('Min Heart Rate'))\n",
        "\n",
        "# https://www.healthline.com/health/dangerous-heart-rate\n",
        "# upper bound: about 100-120 bpm --> 110 bpm\n",
        "# lower bound: 60 bpm\n",
        "# average: betwenn 70 - 100\n",
        "#dfMaxHeartRate = computePersonalizedZScore(dfMaxHeartRate, 'Max Heart Rate')\n",
        "#dfMinHeartRate = computePersonalizedZScore(dfMinHeartRate, 'Min Heart Rate')\n",
        "#dfAvgHeartRate = computePersonalizedZScore(dfAvgHeartRate, 'Average Heart Rate')\n",
        "\n",
        "#dfMaxHeartRate = performanceScore(dfMaxHeartRate, 0, 0, 'Max Heart Rate', upperRisk = 110, zscoreColumn='Max Heart Rate_zscore', rename = 'Max Heart Rate_ps')\n",
        "#dfMaxHeartRate.show(10)\n",
        "#dfMinHeartRate = performanceScore(dfMinHeartRate, 0, 0, 'Min Heart Rate', lowerRisk = 60, zscoreColumn='Min Heart Rate_zscore', rename = 'Min Heart Rate_ps')\n",
        "#dfMinHeartRate.show(10)\n",
        "#dfAvgHeartRate = performanceScore(dfAvgHeartRate, 0, 0, 'Average Heart Rate', lowerRisk = 70, upperRisk = 100, zscoreColumn='Average Heart Rate_zscore', rename = 'Average Heart Rate_ps')\n",
        "#dfAvgHeartRate.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf97f_g-YEgF"
      },
      "outputs": [],
      "source": [
        "#plotPatientPerformanceScore(dfAvgHeartRate, '1065442966', 'Average Heart Rate_ps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bgXL4XJAp1r"
      },
      "source": [
        "## Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJmJT0XFAzQQ"
      },
      "outputs": [],
      "source": [
        "# moved to 'Vitality Conditions'\n",
        "# Diabetes Mellitus (tipo II) - 73211009/ 164971000119101\n",
        "dfDiabetesMellitusNegative = codeExtraction(dfObservations, code='73211009', new_name=\"Diabetes Mellitus\", whichCode='code_coding_code', whichValue='valueCodeableConcept_coding_display')\n",
        "dfDiabetesMellitusPositive = dfConditions.filter(dfConditions.code_coding_code == ' 164971000119101').dropDuplicates(['Patient_id']).select('Patient_id', 'verificationStatus_coding_code').withColumnRenamed('verificationStatus_coding_code', 'Diabetes Mellitus')\n",
        "dfDiabetesMellitus = dfDiabetesMellitusNegative.union(dfDiabetesMellitusPositive)\n",
        "\n",
        "# Conversione della colonna 'Diabetes Mellitus' in binario (1 per \"confirmed\", 0 per \"negative\")\n",
        "dfDiabetesMellitus = dfDiabetesMellitus.withColumn(\"Diabetes_binary\", when(col(\"Diabetes Mellitus\") == \"confirmed\", 1).otherwise(0))\n",
        "\n",
        "# Mostra i risultati della conversione\n",
        "dfDiabetesMellitus.select(\"Patient_id\", \"Diabetes Mellitus\", \"Diabetes_binary\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guohq2doaUUQ"
      },
      "source": [
        "## Cholesterol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE94lx6FaYAt"
      },
      "outputs": [],
      "source": [
        "# HDL [mg/dl] - 28036006\n",
        "# lowerBound: 40 https://www.mayoclinic.org/diseases-conditions/high-blood-cholesterol/in-depth/hdl-cholesterol/art-20046388\n",
        "# desirable: 60\n",
        "dfHDL = codeExtractionWithTime(dfObservations, code='28036006', new_name=\"HDL Cholesterol\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "# LDL [???%???] --> [mg/dl] - 113079009\n",
        "# upperBound: 130 (for borderline HIGH level) https://medlineplus.gov/ldlthebadcholesterol.html\n",
        "# optimal level: < 100\n",
        "\n",
        "dfLDL = codeExtractionWithTime(dfObservations, code='113079009', new_name=\"LDL Cholesterol\", whichCode='code_coding_code', whichValue='valueQuantity_value')\n",
        "#dfHDL = computeZScore(dfHDL, 'HDL Cholesterol')\n",
        "#dfHDL = performanceScore(dfHDL, 0, 0, 'HDL Cholesterol', lowerRisk = 40, zscoreColumn='HDL Cholesterol_zscore', rename = 'HDL Cholesterol_ps')\n",
        "#dfHDL.show(10)\n",
        "\n",
        "#dfLDL = computeZScore(dfLDL, 'LDL Cholesterol')\n",
        "#dfLDL = performanceScore(dfLDL, 0, 0, 'LDL Cholesterol', upperRisk = 130, zscoreColumn='LDL Cholesterol_zscore', rename = 'LDL Cholesterol_ps')\n",
        "#dfLDL.show(10)\n",
        "'''\n",
        "dfHDL.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "dfLDL.orderBy('Patient_id', 'effectiveDateTime').show(30, truncate=False)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i398do-qAriR"
      },
      "source": [
        "## QRS Interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD41tQcIEhYX"
      },
      "outputs": [],
      "source": [
        "# QRS Interval [ms] - 39632005\n",
        "# > 120 ms at risk : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4305520/#:~:text=QRS%20duration%20%E2%89%A5120%20ms,failure%20had%20an%20increased%20mortality.\n",
        "dfQRS = dfObservations.filter(dfObservations.code_coding_code == '39632005') \\\n",
        "          .select('effectiveDateTime', 'Patient_id', 'valueQuantity_value') \\\n",
        "          .orderBy('effectiveDateTime', 'Patient_id')\n",
        "#dfQRS.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D_t1poBdPwm"
      },
      "source": [
        "## BMI and Body Fat %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSUZ9vaOdSEM"
      },
      "outputs": [],
      "source": [
        "# BMI [kg/m2] - 39156-5\n",
        "# normal between 18.5 and 24.9: https://www.nhlbi.nih.gov/health/educational/lose_wt/risk.htm\n",
        "dfBMI = codeExtractionWithTime(dfObservations, '39156-5', 'BMI', 'code_coding_code', 'valueQuantity_value')\n",
        "\n",
        "# Elimina occorrenze sbagliate\n",
        "dfBMI = dfBMI.filter((dfBMI.BMI <= 60) & (dfBMI.BMI >= 10))\n",
        "\n",
        "# Body fat [%{fat}] - 41982-0\n",
        "# normal between 13 and 24: https://www.forbes.com/health/wellness/body-fat-percentage/\n",
        "dfBodyFat = codeExtractionWithTime(dfObservations, '41982-0', 'Body fat percentage', 'code_coding_code', 'valueQuantity_value')\n",
        "dfBodyFat = dfBodyFat.withColumn('effectiveDateTime', to_date('effectiveDateTime')) \\\n",
        "                     .dropDuplicates(['effectiveDateTime', 'Patient_id']) \\\n",
        "\n",
        "                     #.withColumn('Body fat percentage', col('Body fat percentage')/1000) \\\n",
        "\n",
        "#dfBMI.show()\n",
        "#dfBodyFat.show()\n",
        "\n",
        "'''\n",
        "dfBMI = computeZScore(dfBMI, 'BMI')\n",
        "dfBMI = performanceScore(dfBMI, 0, 0, 'BMI', lowerRisk = 18.5, upperRisk = 24.99, zscoreColumn='BMI_zscore', rename = 'BMI_ps')\n",
        "dfBMI.show(10)\n",
        "\n",
        "# g_mean and g_std: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10676745/\n",
        "dfBodyFat = computePersonalizedZScore(dfBodyFat, 'Body fat percentage')\n",
        "dfBodyFat = performanceScore(dfBodyFat, 29.1, 8.7, lowerRisk = 13, upperRisk = 24, zscoreColumn='Body fat percentage_zscore', rename = 'Body fat percentage_ps')\n",
        "dfBodyFat.show(40)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Y7Wik1kL7X"
      },
      "source": [
        "## Standing Blood pressure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tplfGGPkOF7"
      },
      "outputs": [],
      "source": [
        "# Standing Blood pressure mm[Hg]\n",
        "# general code_coding_code: 85354-9\n",
        "dfObservations.filter(dfObservations.component_code_coding_code == '85354-9').select('bodySite_coding_display').distinct()#.show(truncate=False)\n",
        "\n",
        "dfStandingDiastolic = codeExtractionWithTime(dfObservations, '8454-1', 'Standing Diastolic', 'component_code_coding_code', 'component_valueQuantity_value')\n",
        "dfStandingSystolic = codeExtractionWithTime(dfObservations, '8460-8', 'Standing Systolic', 'component_code_coding_code', 'component_valueQuantity_value')\n",
        "dfStandingSystolic.orderBy('Patient_id', 'effectiveDateTime')#.show()\n",
        "\n",
        "dfStandingDiastolic = dfStandingDiastolic.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                              .groupBy('effectiveDateTime', 'Patient_id').agg(mean_('Standing Diastolic').alias('Standing Diastolic'))\n",
        "dfStandingSystolic = dfStandingSystolic.withColumn('effectiveDateTime', to_date('effectiveDateTime'))\\\n",
        "                              .groupBy('effectiveDateTime', 'Patient_id').agg(mean_('Standing Systolic').alias('Standing Systolic'))\n",
        "\n",
        "'''tmp = averageTooCloseMeasurements(dfStandingDiastolic, 'Standing Diastolic', 1)\n",
        "#tmp.show()\n",
        "tmp = normalizeDatsetsLength(tmp, 'Standing Diastolic', 2)\n",
        "tmp.show()'''\n",
        "\n",
        "dfStandingBP = dfStandingSystolic.join(dfStandingDiastolic, ['Patient_id', 'effectiveDateTime'])\n",
        "dfStandingBP.orderBy('Patient_id', 'effectiveDateTime').show(truncate=False)\n",
        "\n",
        "# Standing count\n",
        "'''\n",
        "print(f\"Standing Systolic Count: {dfStandingSystolic.count()}\")\n",
        "print(f\"Standing Diastolic Count: {dfStandingDiastolic.count()}\")\n",
        "print(f\"Total Standing BP Count: {dfStandingBP.count()}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "'''\n",
        "# Chart\n",
        "#dfStandingBPGrouped = prepare_standing_bp_data(dfStandingBP)\n",
        "#standing_bp_chart = create_standing_bp_chart(dfStandingBPGrouped)\n",
        "\n",
        "# Saving results\n",
        "#standing_bp_chart.render(\"standing_bp_chart.html\")\n",
        "#from google.colab import files\n",
        "#files.download(\"standing_bp_chart.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NS_ncljQj8B"
      },
      "source": [
        "## Supine Blood Pressure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7HUFKy5QmS2"
      },
      "outputs": [],
      "source": [
        "# Supine Blood pressure mm[Hg] -\n",
        "dfSupineDiastolic = codeExtractionWithTime(dfObservations, '8455-8', 'Supine Diastolic', 'component_code_coding_code', 'component_valueQuantity_value')\n",
        "dfSupineSystolicBP = codeExtractionWithTime(dfObservations, '8461-6', 'Supine Systolic', 'component_code_coding_code', 'component_valueQuantity_value')\n",
        "\n",
        "dfSupineBP = dfSupineSystolicBP.join(dfSupineDiastolic, ['Patient_id', 'effectiveDateTime'])\n",
        "dfSupineBP.orderBy('Patient_id', 'effectiveDateTime')#.show(truncate=False)\n",
        "\n",
        "# Supine count\n",
        "'''\n",
        "print(f\"Supine Systolic Count: {dfSupineSystolicBP.count()}\")\n",
        "print(f\"Supine Diastolic Count: {dfSupineDiastolic.count()}\")\n",
        "print(f\"Supine BP Count: {dfSupineBP.count()}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "'''\n",
        "dfSupineBPGrouped = prepare_supine_bp_data(dfSupineBP)\n",
        "supine_bp_chart = create_supine_bp_chart(dfSupineBPGrouped)\n",
        "'''\n",
        "# Saving results\n",
        "supine_bp_chart.render(\"supine_bp_chart.html\")\n",
        "from google.colab import files\n",
        "files.download(\"supine_bp_chart.html\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WjQv1yhjWvC"
      },
      "source": [
        "## Vitality conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yEJcK8vBb3o"
      },
      "source": [
        "### Type of conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTGazD4eBdrA"
      },
      "outputs": [],
      "source": [
        "# Extract Conditions and respective Code\n",
        "unique_pairs_df = dfConditions.select('code_coding_display', 'code_coding_code').distinct()\n",
        "\n",
        "# Counting\n",
        "unique_pairs_df_count = unique_pairs_df.count()\n",
        "#print(unique_pairs_df_count)       #46\n",
        "\n",
        "# Printing pairs\n",
        "unique_pairs_df.show(n=unique_pairs_df_count, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coronary Condition"
      ],
      "metadata": {
        "id": "stWMCJj4-aOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN6aOAgDjY5I"
      },
      "outputs": [],
      "source": [
        "vitalityConditionsList = [\n",
        "    ('Coronary arteriosclerosis', '53741008'),\n",
        "]\n",
        "\n",
        "list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == vitalityConditionsList[0][1]) & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "CoronaryConditions = dfObservations.select('Patient_id').distinct().withColumn(vitalityConditionsList[0][0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "for x in vitalityConditionsList[1:]:\n",
        "  list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == x[1]) & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "  CoronaryConditions = CoronaryConditions.withColumn(x[0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "#print(CoronaryConditions.count())\n",
        "#CoronaryConditions.orderBy(col(\"Coronary arteriosclerosis\").desc()).show(30, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cardiovascular Condition"
      ],
      "metadata": {
        "id": "N-JKhq6F-eXH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7WYWfG2lvOL"
      },
      "outputs": [],
      "source": [
        "vitalityConditionsList2 = [\n",
        "    ('H/O: cardiovascular disease', '266995000'),\n",
        "]\n",
        "\n",
        "list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == vitalityConditionsList2[0][1]) & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "HOcardioConditions = dfObservations.select('Patient_id').distinct().withColumn(vitalityConditionsList2[0][0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "for x in vitalityConditionsList2[1:]:\n",
        "  list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == x[1]) & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "  HOcardioConditions = HOcardioConditions.withColumn(x[0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "#print(HOcardioConditions.count())\n",
        "#HOcardioConditions.orderBy(col(\"H/O: cardiovascular disease\").desc()).show(30, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Myocardial infarction"
      ],
      "metadata": {
        "id": "4ThKSqS0-kh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vitalityConditionsList3 = [\n",
        "    ('Myocardial infarction due to atherothrombotic coronary artery disease', '726499301000119105'),\n",
        "]\n",
        "\n",
        "list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == vitalityConditionsList3[0][1]) & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "MyocardialConditions = dfObservations.select('Patient_id').distinct().withColumn(vitalityConditionsList3[0][0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "for x in vitalityConditionsList3[1:]:\n",
        "  list_patient = dfConditions\\\n",
        "                      .filter((dfConditions.code_coding_code == x[1]) & \\\n",
        "                      (dfConditions.verificationStatus_coding_code == 'confirmed')) \\\n",
        "                      .distinct().rdd.map(lambda x: x.Patient_id).collect()\n",
        "  MyocardialConditions = MyocardialConditions.withColumn(x[0], EncodeInList('Patient_id', lit(list_patient)))\n",
        "\n",
        "#print(MyocardialConditions.count())\n",
        "#MyocardialConditions.orderBy(col(\"Myocardial infarction due to atherothrombotic coronary artery disease\").desc()).show(30, truncate=False)"
      ],
      "metadata": {
        "id": "hYZdImfh-m_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNHptDpO57_y"
      },
      "source": [
        "# PREDICTIVE ANALYTIC TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNR8SQilHw9G"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM2S8UGH9yGf"
      },
      "source": [
        "### Age Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl6UjKgd90Eo"
      },
      "outputs": [],
      "source": [
        "#dfAge.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAhTM7R_OEd1"
      },
      "source": [
        "### Gender Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFV66FEc594T"
      },
      "outputs": [],
      "source": [
        "\n",
        "dfAgeGender = dfAge.join(dfGender, on=\"Patient_id\")\n",
        "\n",
        "# Creating column 'Gender_Risk' by following conditions\n",
        "dfResult = dfAgeGender.withColumn(\n",
        "    \"Gender_Risk\",\n",
        "    when((col(\"Gender\") == 'Male') |\n",
        "         (col(\"Gender\") == 'Transgender male') |\n",
        "         ((col(\"Gender\") == 'Female') & (col(\"Age\") >= 50)) |\n",
        "         ((col(\"Gender\") == 'Transgender female') & (col(\"Age\") >= 50)), 1)\n",
        "    .otherwise(0)\n",
        ")\n",
        "\n",
        "dfGender_Risk = dfResult.select('Patient_id', 'Gender_Risk').withColumnRenamed(\"Gender_Risk\", \"Gender\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYjTW49EfIqH"
      },
      "source": [
        "### Systolic Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djPMvw5zfTP-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Raggruppa per Patient_id e calcola la media dei valori di Standing Systolic e Standing Diastolic\n",
        "dfAggregated = dfStandingBP.groupBy(\"Patient_id\").agg(\n",
        "    avg(\"Standing Systolic\").alias(\"Systolic\"),\n",
        "    avg(\"Standing Diastolic\").alias(\"Diastolic\")\n",
        ")\n",
        "\n",
        "dfBP_Systolic_Risk = dfAggregated.select('Patient_id', 'Systolic').withColumnRenamed(\"Systolic\", \"Blood Pressure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cholesterol"
      ],
      "metadata": {
        "id": "KIKOihc7MoOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfLDL_Risk = dfLDL.groupBy(\"Patient_id\").agg(\n",
        "    avg(\"LDL Cholesterol\").alias(\"LDL Cholesterol\")\n",
        ")\n",
        "\n",
        "'''\n",
        "dfHDL_Risk = dfHDL.groupBy(\"Patient_id\").agg(\n",
        "    avg(\"HDL Cholesterol\").alias(\"HDL Cholesterol\")\n",
        ")\n",
        "\n",
        "#dfHDL_Risk = dfHDL_Risk.withColumn(\"HDL Cholesterol\",\n",
        "                                   #when(col(\"HDL Cholesterol\") <52, 1)\n",
        "                                   #.otherwise(0))\n",
        "\n",
        "\n",
        "dfCholesterol_Risk = dfLDL_Risk.join(dfHDL_Risk, on=\"Patient_id\", how=\"outer\")\n",
        "\n",
        "dfCholesterol_Risk = dfCholesterol_Risk.select('Patient_id', 'LDL Cholesterol', 'HDL Cholesterol')\n",
        "'''\n",
        "\n",
        "dfCholesterol_Risk = dfLDL_Risk.select('Patient_id', 'LDL Cholesterol')"
      ],
      "metadata": {
        "id": "U7nTRTbLMp-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Physical Activity"
      ],
      "metadata": {
        "id": "XTAu3uuOsZIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfPhysicalActivity = dfNumberSteps.groupBy(\"Patient_id\").agg(\n",
        "    avg(\"Number of Steps\").alias(\"Physical Activity\")\n",
        ")\n",
        "\n",
        "dfPhysicalActivity = dfPhysicalActivity.select('Patient_id', 'Physical Activity')\n",
        "\n",
        "#dfPhysicalActivity.show()"
      ],
      "metadata": {
        "id": "vKvJhGz4sa_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8TrSO_xGfWP"
      },
      "source": [
        "### BMI Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wVRKxxFGh5r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Aggrega dati multipli\n",
        "dfBMI_Risk = dfBMI.groupBy(\"Patient_id\").agg(\n",
        "    avg(\"BMI\").alias(\"BMI\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Q6VDQpMsuj"
      },
      "source": [
        "### Smoke Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du3weOjVMvqc"
      },
      "outputs": [],
      "source": [
        "dfSmoker = dfSmoker.withColumn(\"Smoking Status\",\n",
        "                                when(col(\"Smoking Status\") == \"Never smoker\", 0)\n",
        "                                #.when(col(\"Smoking Status\") == \"Former smoker\", 1)\n",
        "                                #.when(col(\"Smoking Status\") == \"Current some day smoker\", 1)\n",
        "                                .otherwise(2)\n",
        "                                ).withColumnRenamed(\"Smoking Status\", \"Smoker\")\n",
        "\n",
        "dfSmoker_Risk = dfSmoker.select('Patient_id', 'Smoker')\n",
        "\n",
        "#dfSmoker_Risk.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qdUNGULyJOi"
      },
      "source": [
        "### Diabetes Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmPWVSOhyLu7"
      },
      "outputs": [],
      "source": [
        "# Rinomina la colonna Diabetes_binary in Diabetes_Risk\n",
        "dfDiabetesMellitus = dfDiabetesMellitus.withColumnRenamed(\"Diabetes_binary\", \"Diabetic\")\n",
        "\n",
        "dfDiabetes_Risk = dfDiabetesMellitus.select('Patient_id', 'Diabetic')\n",
        "\n",
        "#print(dfDiabetes_Risk.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tcmRG9FmCwi"
      },
      "source": [
        "### Coronary Arteriosclerosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsDcVTw3mFvV"
      },
      "outputs": [],
      "source": [
        "# Rinomina colonna\n",
        "dfCoronaryArteriosclerosis = CoronaryConditions.withColumnRenamed(\"Coronary arteriosclerosis\", \"Coronary Arteriosclerosis\")\n",
        "\n",
        "dfCoronaryArteriosclerosis = dfCoronaryArteriosclerosis.select('Patient_id', 'Coronary Arteriosclerosis')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Myocardial"
      ],
      "metadata": {
        "id": "GR2j_w-QFYqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfMyocardialConditions = MyocardialConditions.withColumnRenamed(\"Myocardial infarction due to atherothrombotic coronary artery disease\", \"Suffered from Heart Attack\")\n",
        "\n",
        "dfMyocardialConditions = dfMyocardialConditions.select('Patient_id', 'Suffered from Heart Attack')\n",
        "\n",
        "#dfMyocardialConditions.show()"
      ],
      "metadata": {
        "id": "ZoGHCboeFcLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrNN64Ov_zPh"
      },
      "source": [
        "### Cardiovascular Disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlbS4Km_2G4"
      },
      "outputs": [],
      "source": [
        "# Rinomina colonna\n",
        "dfCardiovascularDisease = HOcardioConditions.withColumnRenamed(\"H/O: cardiovascular disease\", \"Cardiovascular_Disease\")\n",
        "\n",
        "dfCardiovascularDisease = dfCardiovascularDisease.select('Patient_id', 'Cardiovascular_Disease')\n",
        "\n",
        "#print(dfCardiovascularDisease.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Dataframes"
      ],
      "metadata": {
        "id": "4Vgh2V4Tg63D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe finale"
      ],
      "metadata": {
        "id": "DiEcTOUs9a1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging all dataframes on 'Patient_id'\n",
        "dfFinal = dfAge \\\n",
        "    .join(dfBP_Systolic_Risk, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfBMI_Risk, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfSmoker_Risk, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfGender_Risk, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfDiabetes_Risk, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfCoronaryArteriosclerosis, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfMyocardialConditions, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfCardiovascularDisease, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfCholesterol_Risk, \"Patient_id\", \"outer\") \\\n",
        "    .join(dfPhysicalActivity, \"Patient_id\", \"outer\")\n",
        "\n",
        "# Drop Duplicates\n",
        "dfFinal = dfFinal.dropDuplicates(['Patient_id'])\n",
        "\n",
        "# Data Imputation\n",
        "dfFinal = replace_null_with_zero(dfFinal)\n",
        "\n",
        "#dfFinal_Count = dfFinal.count()\n",
        "\n",
        "#dfFinal.show(20, truncate=False)\n",
        "\n",
        "limited_dfFinal = dfFinal.limit(20).toPandas()\n",
        "\n",
        "styled_limited_df = limited_dfFinal.style.set_table_styles(\n",
        "    [{'selector': 'thead th', 'props': [('background-color', 'lightblue'), ('color', 'black'), ('text-align', 'center')]}]\n",
        ").set_properties(**{'text-align': 'center', 'border': '1px solid black'})\n",
        "\n",
        "# Mostra la tabella\n",
        "#display(styled_limited_df)\n",
        "\n",
        "'''\n",
        "# ----- Dividi dfFinal in file multipli per l'upload su GitHub -----\n",
        "\n",
        "# Trasformo in pandas\n",
        "dfFinal_pd = dfFinal.toPandas()\n",
        "\n",
        "# Numero di righe per ogni file\n",
        "chunk_size = 200\n",
        "\n",
        "# Numero totale di file necessari\n",
        "num_chunks = len(dfFinal_pd) // chunk_size + 1\n",
        "\n",
        "# Salva ogni chunk in un file separato\n",
        "for i in range(num_chunks):\n",
        "    chunk = dfFinal_pd.iloc[i*chunk_size:(i+1)*chunk_size]\n",
        "    chunk.to_parquet(f'chunk_{i}.parquet')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Scarica i file chunk\n",
        "for i in range(num_chunks):\n",
        "    files.download(f'/content/chunk_{i}.parquet')\n",
        "'''"
      ],
      "metadata": {
        "id": "MRrYDBk79dmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intrinsic Capacity"
      ],
      "metadata": {
        "id": "S6AFOhFIhAv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfIC_Final = dfIC.groupBy('Patient_id').agg(\n",
        "    avg('Intrinsic Capacity_imputed').alias('Intrinsic Capacity')\n",
        ")\n",
        "\n",
        "dfIC_Final = dfIC_Final.toPandas()\n",
        "\n",
        "#print(dfIC_Filtered.count())\n",
        "#dfIC_Filtered.show()"
      ],
      "metadata": {
        "id": "Qci2YOmthDOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST 2"
      ],
      "metadata": {
        "id": "SEm9QdJp1wUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run 2"
      ],
      "metadata": {
        "id": "gL3ZgK9LJNNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Convertiamo in Pandas per l'uso in scikit-learn\n",
        "dfFinal = dfFinal.toPandas()\n",
        "\n",
        "# Defining X for features and y for true label\n",
        "X = dfFinal[['Physical Activity', 'Diabetic',\n",
        "             'Coronary Arteriosclerosis',\n",
        "             'Suffered from Heart Attack',\n",
        "             'LDL Cholesterol', 'Blood Pressure',\n",
        "             'BMI', 'Smoker', 'Gender', 'Age']]\n",
        "y = dfFinal['Cardiovascular_Disease']\n",
        "\n",
        "# Pipeline to combine Standardization and Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Setting k-fold cross-validation with k=5\n",
        "k = 5\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "model = pipeline.fit(X, y)\n",
        "\n",
        "# Making predictions\n",
        "predictions = pipeline.predict(X)\n",
        "probabilities = pipeline.predict_proba(X)[:, 1]\n",
        "\n",
        "# Final DataFrame with patient_id and results\n",
        "results_df = pd.DataFrame({\n",
        "    'Patient_id': patient_ids,\n",
        "    'Predicted_Risk': predictions,\n",
        "    'Probability_Percent': probabilities * 100,\n",
        "})\n",
        "\n",
        "dfRisk_Final = results_df[['Patient_id', 'Predicted_Risk']]\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Mostriamo i risultati senza la colonna \"cardiovascular_disease\"\n",
        "print(results_df[['Patient_id', 'Predicted_Risk', 'Probability_Percent']])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "23u4RnZ31zcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Results 2"
      ],
      "metadata": {
        "id": "gVdRAc5Z9scd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "limited_df = results_df[['Patient_id', 'Predicted_Risk', 'Probability_Percent']].head(13)\n",
        "\n",
        "styled_df = limited_df[['Patient_id', 'Predicted_Risk', 'Probability_Percent']].style.set_table_styles(\n",
        "    [{'selector': 'thead th', 'props': [('background-color', 'lightblue'),('color', 'black'), ('text-align', 'center')]}]\n",
        ").set_properties(**{'text-align': 'center', 'border': '1px solid black'})\n",
        "\n",
        "# Mostra la tabella\n",
        "display(styled_df)\n",
        "\n",
        "# Eseguiamo la cross-validation\n",
        "results = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# -------- Media delle performance --------\n",
        "\n",
        "# Crea un DataFrame per media e varianza delle accuracy\n",
        "accuracy_df = pd.DataFrame({\n",
        "    'Metric': ['Mean Accuracy', 'Accuracy Variance'],\n",
        "    'Value': [np.mean(results), np.std(results)]\n",
        "})\n",
        "\n",
        "# Stile della tabella\n",
        "styled_accuracy_df = accuracy_df.style.set_table_styles(\n",
        "    [{'selector': 'thead th', 'props': [('background-color', 'lightblue'), ('color', 'black'), ('text-align', 'center')]}]\n",
        ").set_properties(**{'text-align': 'center', 'border': '1px solid black'})\n",
        "\n",
        "# Mostra la tabella\n",
        "display(styled_accuracy_df)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "features = ['Physical Activity', 'Diabetic', 'Coronary Arteriosclerosis', 'Suffered from Heart Attack', 'LDL Cholesterol', 'Blood Pressure', 'BMI', 'Smoker', 'Gender', 'Age']\n",
        "\n",
        "# Estrai i coefficienti delle feature\n",
        "coefficients = model.named_steps['model'].coef_[-1]  # Coefficienti\n",
        "\n",
        "# Crea un DataFrame con i coefficienti\n",
        "coefficients_pd = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "#print(coefficients_pd)\n",
        "\n",
        "# Stile per la tabella dei coefficienti\n",
        "styled_coefficients_df = coefficients_pd.style.set_table_styles(\n",
        "    [{'selector': 'thead th', 'props': [('background-color', 'lightblue'), ('color', 'black'),('text-align', 'center')]}]\n",
        ").set_properties(**{'text-align': 'center', 'border': '1px solid black'})\n",
        "\n",
        "# Mostra la tabella dei coefficienti\n",
        "display(styled_coefficients_df)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualizzazione dei coefficienti\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(coefficients_pd['Feature'], coefficients_pd['Coefficient'], color='skyblue')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Coefficients of Features in Logistic Regression')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualizzazione delle probabilità predette\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(results_df['Probability_Percent'], bins=35, color='salmon', edgecolor='black')\n",
        "plt.xlabel('Probability Percent')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Cardiovascular Risk Probabilities')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(results_df['Cardiovascular_Disease'], results_df['Probability_Percent'])\n",
        "auc_score = roc_auc_score(results_df['Cardiovascular_Disease'], results_df['Probability_Percent'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'AUC = {auc_score:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(results_df['Cardiovascular_Disease'], results_df['Probability_Percent'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall, precision, marker='.', color='green')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "m6qjoNTf9uKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Study"
      ],
      "metadata": {
        "id": "-SCSNeE7l7r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "dfCorrelation = pd.merge(dfRisk_Final, dfIC_Final, on='Patient_id')\n",
        "\n",
        "# Pearson Correlation\n",
        "pearson_corr = dfCorrelation['Predicted_Risk'].corr(dfCorrelation\n",
        "                        ['Intrinsic Capacity'], method='pearson')\n",
        "\n",
        "# Spearman Correlation\n",
        "spearman_corr = dfCorrelation['Predicted_Risk'].corr(dfCorrelation\n",
        "                        ['Intrinsic Capacity'], method='spearman')\n",
        "\n",
        "# Create a DataFrame containing Correlation's results\n",
        "correlation_results = {\n",
        "    'Correlation': ['Pearson', 'Spearman'],\n",
        "    'Result': [pearson_corr, spearman_corr]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "correlation_df = pd.DataFrame(correlation_results)\n",
        "\n",
        "# Stile della tabella\n",
        "styled_correlation_df = correlation_df.style.set_table_styles(\n",
        "    [{'selector': 'thead th', 'props': [('background-color', 'lightblue'), ('color', 'black'), ('text-align', 'center')]}]\n",
        ").set_properties(**{'text-align': 'center', 'border': '1px solid black'})\n",
        "\n",
        "# Mostra la tabella\n",
        "display(styled_correlation_df)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Scatter plot with regression line\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Intrinsic Capacity', y='Predicted_Risk', data=dfCorrelation, s=100)\n",
        "sns.regplot(x='Intrinsic Capacity', y='Predicted_Risk', data=dfCorrelation, scatter=False, color='red')\n",
        "\n",
        "# Adding labels\n",
        "plt.title('Scatter Plot of Intrinsic Capacity vs Predicted Cardiovascular Risk')\n",
        "plt.xlabel('Intrinsic Capacity')\n",
        "plt.ylabel('Predicted Risk')\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gNpyG-dGl_Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5roeOENcH5_G"
      },
      "source": [
        "# [ANALAYTICS](https://unimi2013-my.sharepoint.com/:x:/g/personal/mattia_occhipinti_unimi_it/EfH2kwGHllVNvysPWuqfUNIB3r6w6jtX-t84YpbyQN5e-A?rtime=r3GpQFDP3Eg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POMdcCJmEaCv"
      },
      "source": [
        "## Analytics 1.5 - Age Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ioiD7kmEgvQ"
      },
      "outputs": [],
      "source": [
        "# Age distribution of recruited study participants\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def calculate_age_group_statistics(df, age_col=\"Age\"):\n",
        "    # Defining groups\n",
        "    bins = [29, 39, 49, 49, 59, 69, 79, 100]\n",
        "    labels = [\"0-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80+\"]\n",
        "\n",
        "    # New column for groups\n",
        "    df = df.withColumn(\"AgeGroup\",\n",
        "                       F.when((F.col(age_col) >= 0) & (F.col(age_col) <= 29), \"0-29\")\n",
        "                       .when((F.col(age_col) >= 30) & (F.col(age_col) <= 39), \"30-39\")\n",
        "                       .when((F.col(age_col) >= 40) & (F.col(age_col) <= 49), \"40-49\")\n",
        "                       .when((F.col(age_col) >= 50) & (F.col(age_col) <= 59), \"50-59\")\n",
        "                       .when((F.col(age_col) >= 60) & (F.col(age_col) <= 69), \"60-69\")\n",
        "                       .when((F.col(age_col) >= 70) & (F.col(age_col) <= 79), \"70-79\")\n",
        "                       .otherwise(\"80+\")\n",
        "                       )\n",
        "\n",
        "    # Calculating participants, mean and dev for every group\n",
        "    result = df.groupBy(\"AgeGroup\") \\\n",
        "               .agg(\n",
        "                   F.count(\"*\").alias(\"N\"),\n",
        "                   F.mean(age_col).alias(\"Mean\"),\n",
        "                   F.stddev(age_col).alias(\"StdDev\")\n",
        "               ) \\\n",
        "               .orderBy(\"AgeGroup\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Calling the function\n",
        "age_statistics = calculate_age_group_statistics(dfAge)\n",
        "\n",
        "# Showing results\n",
        "#age_statistics.show(truncate=False)\n",
        "#print(f\"Number of active patients: {num_active_patients}\")\n",
        "\n",
        "# Creating E-Chart\n",
        "\n",
        "def create_age_distribution_chart(df: DataFrame):\n",
        "    # Converting PySpark DataFrame into DataFrame Pandas\n",
        "    df_pd = df.toPandas()\n",
        "\n",
        "    # Data Extraction\n",
        "    age_groups = df_pd['AgeGroup'].tolist()\n",
        "    n_participants = df_pd['N'].tolist()\n",
        "    means = df_pd['Mean'].tolist()\n",
        "    std_devs = df_pd['StdDev'].fillna(0).tolist()  # Fills null with 0 for empty std dev\n",
        "\n",
        "    # Formatting Labels\n",
        "    labels = [f\"Mean: {means[i]:.2f}\\nStd Dev: {std_devs[i]:.2f}\\nN: {n_participants[i]}\" for i in range(len(age_groups))]\n",
        "\n",
        "    # Graph Creation\n",
        "    bar = (\n",
        "        Bar()\n",
        "        .add_xaxis(age_groups)\n",
        "        .add_yaxis(\n",
        "            \"# of Participants\",\n",
        "            n_participants,\n",
        "            label_opts=opts.LabelOpts(\n",
        "                is_show=True,\n",
        "                position=\"top\",\n",
        "                formatter=JsCode(\n",
        "                    \"\"\"\n",
        "                    function(x) {\n",
        "                        const labels = %s;\n",
        "                        return labels[x.dataIndex];\n",
        "                    }\n",
        "                    \"\"\" % labels\n",
        "                )\n",
        "            ),\n",
        "            category_gap=\"30%\"\n",
        "        )\n",
        "        .set_global_opts(\n",
        "            title_opts=opts.TitleOpts(title=\"Profiling of Patients\"),\n",
        "            xaxis_opts=opts.AxisOpts(name=\"Age Groups\", name_textstyle_opts=opts.TextStyleOpts(font_weight='bold'), axislabel_opts=opts.LabelOpts(font_weight='bold')),\n",
        "            yaxis_opts=opts.AxisOpts(name=\"# of Participants\", max_=1200, axislabel_opts=opts.LabelOpts(font_weight='bold')),\n",
        "            tooltip_opts=opts.TooltipOpts(trigger=\"axis\", axis_pointer_type=\"shadow\", formatter=lambda params: f\"<b>{params.name}</b><br>{labels[params.dataIndex]}\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Graph Render\n",
        "    return bar\n",
        "\n",
        "# Calling the functions\n",
        "bar_chart = create_age_distribution_chart(age_statistics)\n",
        "\n",
        "# Saving results\n",
        "bar_chart.render(\"age_distribution.html\")\n",
        "from google.colab import files\n",
        "files.download(\"age_distribution.html\")\n",
        "\n",
        "# Showing results\n",
        "#bar_chart.render_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V4CdSDkSQYj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm7tqHLUSRlq"
      },
      "source": [
        "## Analytics 2.5 - Comorbidities profiles per age group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRHRhWXFSU6r"
      },
      "outputs": [],
      "source": [
        "dfFinal = dfVitalityConditions.join(dfAge, on='Patient_id', how='inner')\n",
        "dfFinal = dfFinal.select('Patient_id', 'Age', *dfVitalityConditions.columns[1:])\n",
        "\n",
        "# Calling Age Distribution and Chart functions\n",
        "dfFinal = create_age_groups(dfFinal)\n",
        "dfSummary = sum_patients_per_condition(dfFinal)\n",
        "age_groups, data = prepare_data_for_echarts(dfSummary)\n",
        "bar_chart = create_stacked_bar_chart(age_groups, data)\n",
        "\n",
        "# Saving results\n",
        "bar_chart.render(\"age_distribution_conditions.html\")\n",
        "from google.colab import files\n",
        "files.download(\"age_distribution_conditions.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIrRyI3AlTO"
      },
      "source": [
        "## TEST 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run 1"
      ],
      "metadata": {
        "id": "f2Dx1PVlJUq3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3AI6rEFAnam"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Selezionare le feature e il target\n",
        "feature_columns = ['Physical Activity', 'Diabetic', 'Cardiac Patient', 'Suffered from Heart Attack', 'LDL Cholesterol', 'Blood Pressure', 'BMI', 'Smoker', 'Gender', 'Age']\n",
        "target_column = 'Cardiovascular_Disease'\n",
        "\n",
        "# ---------------\n",
        "\n",
        "# Calcolo la frequenza della classe\n",
        "class_frequency = dfFinal.groupBy(target_column).count().collect()\n",
        "\n",
        "# Calcolo il peso della classe\n",
        "total_sample = dfFinal_Count\n",
        "class_weights = {row[target_column]: total_sample / row['count'] for row in class_frequency}\n",
        "\n",
        "# Creo una colonna di pesi nel DataFrame\n",
        "dfFinal = dfFinal.withColumn('weights', F.when(dfFinal['Cardiovascular_Disease'] == 0, class_weights[0]).otherwise(class_weights[1]))\n",
        "\n",
        "# ---------------\n",
        "\n",
        "# Step 1: Assembla le feature in un unico vettore\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\")\n",
        "\n",
        "# Step 2: Normalizzazione (Z-score scaling)\n",
        "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features_scaled\", withMean=True, withStd=True)\n",
        "\n",
        "# Step 3: Modello di regressione logistica\n",
        "lr = LogisticRegression(featuresCol=\"features_scaled\", labelCol=target_column, predictionCol=\"Cardio_Risk\", probabilityCol=\"probability\", weightCol='weights')\n",
        "\n",
        "# Step 4: Pipeline di trasformazione e modello\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "# Dividi i dati in training e test (80% training, 20% test)\n",
        "train_df, test_df = dfFinal.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 5: Allena il modello\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "# Step 6: Prevedi il rischio cardiovascolare\n",
        "predictions = model.transform(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsPWgu8fEdEL"
      },
      "source": [
        "### Show Results 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33lmTMaREk6C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 7: Estrai i coefficienti delle feature\n",
        "coefficients = model.stages[-1].coefficients.toArray()\n",
        "intercept = model.stages[-1].intercept\n",
        "\n",
        "# Converte i coefficienti in formato Pandas DataFrame\n",
        "coefficients_pd = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "print(coefficients_pd)\n",
        "\n",
        "# Salvo i coefficienti\n",
        "#coefficients_pd.to_parquet('coefficients.parquet', index=False)\n",
        "#from google.colab import files\n",
        "#files.download('coefficients.parquet')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Definisci una funzione UDF per estrarre la probabilità per la classe positiva (1)\n",
        "def extract_probability(probability_vector):\n",
        "    # Assumendo che la classe positiva sia sempre l'indice 1\n",
        "    return float(probability_vector[1])\n",
        "\n",
        "# Registra la funzione UDF\n",
        "extract_probability_udf = udf(extract_probability, DoubleType())\n",
        "\n",
        "# Applica l'UDF per ottenere la probabilità percentuale\n",
        "result_df = predictions.withColumn(\"probability_percent\", extract_probability_udf(F.col(\"probability\")) * 100)\n",
        "\n",
        "#HDL_Corr = result_df.select('HDL Cholesterol', 'Cardio_Risk').corr('HDL Cholesterol', 'Cardio_Risk')\n",
        "#print('Correlazione tra HDL e rischio: ', HDL_Corr)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Seleziona le colonne finali per il risultato\n",
        "result_df = result_df.select(\"Patient_id\", \"Cardio_Risk\", \"probability_percent\", \"Cardiovascular_Disease\")\n",
        "\n",
        "# Converti il DataFrame PySpark in Pandas per la visualizzazione\n",
        "final_results_pd = result_df.toPandas()\n",
        "\n",
        "# Scarico il DataFrame in parquet\n",
        "#final_results_pd.to_parquet('final_results.parquet', index=False)\n",
        "#from google.colab import files\n",
        "#files.download('final_results.parquet')\n",
        "\n",
        "# Visualizzazione dei coefficienti\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(coefficients_pd['Feature'], coefficients_pd['Coefficient'], color='skyblue')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Coefficients of Features in Logistic Regression')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualizzazione delle probabilità predette\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(final_results_pd['probability_percent'], bins=35, color='salmon', edgecolor='black')\n",
        "plt.xlabel('Probability Percent')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Cardiovascular Risk Probabilities')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(final_results_pd['Cardiovascular_Disease'], final_results_pd['probability_percent'])\n",
        "auc_score = roc_auc_score(final_results_pd['Cardiovascular_Disease'], final_results_pd['probability_percent'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'AUC = {auc_score:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(final_results_pd['Cardiovascular_Disease'], final_results_pd['probability_percent'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall, precision, marker='.', color='green')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"###################################################################\")\n",
        "print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zD_BzkrHG4KR",
        "yuuCFILtFMOm",
        "msNkIpsyaYzn",
        "NasG-x_-5xwz",
        "56i9QIDtYXJA",
        "YASDF-6tAi1h",
        "k_ujmYbEJuZa",
        "5roeOENcH5_G",
        "WAhcGKn_GYL0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}